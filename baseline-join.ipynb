{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/glove_200_angular/glove-200-angular.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/glove_200_angular/clusters_glove_200_angular.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/glove_200_angular/ground_truth_glove_200_angular_0_4_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/fashion_mnist_784_euclidean/fashion-mnist-784-euclidean.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/fashion_mnist_784_euclidean/clusters_fashion_mnist_784_euclidean.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/fashion_mnist_784_euclidean/ground_truth_fashion_mnist_784_euclidean_0_0_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "    \n",
    "# data = h5py.File('/home/sunji/ANN/nytimes_256_angular/nytimes-256-angular.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/nytimes_256_angular/clusters_nytimes_256_angular.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/nytimes_256_angular/ground_truth_nytimes_256_angular_0_4_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "    \n",
    "# data = h5py.File('/home/sunji/ANN/sift_128_euclidean/sift-128-euclidean.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/sift_128_euclidean/clusters_sift_128_euclidean.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/sift_128_euclidean/ground_truth_sift_128_euclidean_0_0_0_2.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/kosarak_jaccard/kosarak-jaccard.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/kosarak_jaccard/clusters_kosarak_jaccard.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/kosarak_jaccard/ground_truth_kosarak_jaccard_0_9_1_0.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "    \n",
    "data = h5py.File('/home/sunji/ANN/gist_960_euclidean/gist-960-euclidean.hdf5', 'r')\n",
    "data_train = np.array(data['train'])\n",
    "data_test = np.array(data['test'])\n",
    "with open('/home/sunji/ANN/gist_960_euclidean/clusters_gist_960_euclidean.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)\n",
    "with open('/home/sunji/ANN/gist_960_euclidean/ground_truth_gist_960_euclidean_0_0_0_1.pkl', 'rb') as f:\n",
    "    ground_truth_total = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0459, 0.0439, 0.0251, 0.0318, 0.0159, 0.0564, 0.0335, 0.0273,\n",
       "       0.0156, 0.0627, 0.0504, 0.0116, 0.0289, 0.0432, 0.0388, 0.0282,\n",
       "       0.0405, 0.0417, 0.0309, 0.0338, 0.0165, 0.0291, 0.0245, 0.0208,\n",
       "       0.0207, 0.0727, 0.0386, 0.0145, 0.0347, 0.0462, 0.0238, 0.0333,\n",
       "       0.0616, 0.0418, 0.0344, 0.0448, 0.0221, 0.0348, 0.0275, 0.0319,\n",
       "       0.0445, 0.1036, 0.0365, 0.0168, 0.0539, 0.0554, 0.0224, 0.0432,\n",
       "       0.1612, 0.0764, 0.0892, 0.1059, 0.0974, 0.057 , 0.064 , 0.072 ,\n",
       "       0.1108, 0.1132, 0.0399, 0.035 , 0.0914, 0.0654, 0.0676, 0.0481,\n",
       "       0.2254, 0.0976, 0.1954, 0.1424, 0.1787, 0.0932, 0.0989, 0.0909,\n",
       "       0.1794, 0.0632, 0.104 , 0.0889, 0.1972, 0.0874, 0.1693, 0.1058,\n",
       "       0.0982, 0.0606, 0.1163, 0.0678, 0.0728, 0.065 , 0.029 , 0.0384,\n",
       "       0.0692, 0.0409, 0.0631, 0.0669, 0.1183, 0.0573, 0.0792, 0.1229,\n",
       "       0.0317, 0.0608, 0.0381, 0.0357, 0.0282, 0.0554, 0.0146, 0.0199,\n",
       "       0.0332, 0.0343, 0.0357, 0.0265, 0.0308, 0.0283, 0.0285, 0.0467,\n",
       "       0.0312, 0.0634, 0.0273, 0.0298, 0.0189, 0.0766, 0.0253, 0.0241,\n",
       "       0.0218, 0.0432, 0.0398, 0.0132, 0.0252, 0.0278, 0.0262, 0.0261,\n",
       "       0.0442, 0.0496, 0.0578, 0.0428, 0.0215, 0.077 , 0.0415, 0.0274,\n",
       "       0.0247, 0.0802, 0.0313, 0.0157, 0.0308, 0.0564, 0.0409, 0.0171,\n",
       "       0.0445, 0.0431, 0.0398, 0.0338, 0.015 , 0.0268, 0.0264, 0.0234,\n",
       "       0.0342, 0.0785, 0.044 , 0.0111, 0.0373, 0.0732, 0.0441, 0.0267,\n",
       "       0.062 , 0.0705, 0.052 , 0.0588, 0.0347, 0.0398, 0.0434, 0.0626,\n",
       "       0.0348, 0.0771, 0.0439, 0.0277, 0.0688, 0.1063, 0.0426, 0.0315,\n",
       "       0.1182, 0.108 , 0.1233, 0.1331, 0.1333, 0.0894, 0.084 , 0.147 ,\n",
       "       0.1676, 0.0873, 0.0449, 0.0598, 0.1058, 0.1296, 0.1083, 0.0598,\n",
       "       0.3077, 0.171 , 0.2578, 0.1343, 0.3025, 0.148 , 0.1543, 0.115 ,\n",
       "       0.3003, 0.129 , 0.1667, 0.1124, 0.2835, 0.1885, 0.268 , 0.1088,\n",
       "       0.1394, 0.0846, 0.1385, 0.0723, 0.1257, 0.1025, 0.0347, 0.0402,\n",
       "       0.096 , 0.0779, 0.0715, 0.1003, 0.1399, 0.1198, 0.1459, 0.1709,\n",
       "       0.0286, 0.0901, 0.0579, 0.0447, 0.0259, 0.072 , 0.027 , 0.0172,\n",
       "       0.0401, 0.0367, 0.0308, 0.0446, 0.0333, 0.0428, 0.0485, 0.0842,\n",
       "       0.0423, 0.0993, 0.0507, 0.0226, 0.0334, 0.1118, 0.0455, 0.0215,\n",
       "       0.034 , 0.0626, 0.0287, 0.0151, 0.0331, 0.0358, 0.0273, 0.0148,\n",
       "       0.0481, 0.0572, 0.1005, 0.041 , 0.0327, 0.039 , 0.0301, 0.0316,\n",
       "       0.0348, 0.0455, 0.0162, 0.0291, 0.0381, 0.0462, 0.0711, 0.0388,\n",
       "       0.1661, 0.0992, 0.131 , 0.1682, 0.2028, 0.1029, 0.1002, 0.1855,\n",
       "       0.1282, 0.0897, 0.0436, 0.0749, 0.1315, 0.1428, 0.0925, 0.0733,\n",
       "       0.3956, 0.2322, 0.2294, 0.3246, 0.4046, 0.2421, 0.1451, 0.292 ,\n",
       "       0.3793, 0.2134, 0.1256, 0.2624, 0.3962, 0.2613, 0.2024, 0.2949,\n",
       "       0.1559, 0.1304, 0.1246, 0.1057, 0.1153, 0.1169, 0.0454, 0.0656,\n",
       "       0.1537, 0.0689, 0.0724, 0.1378, 0.1997, 0.1151, 0.1348, 0.1879,\n",
       "       0.0378, 0.0295, 0.0267, 0.0348, 0.0299, 0.1118, 0.0764, 0.0252,\n",
       "       0.0216, 0.0718, 0.0883, 0.024 , 0.0255, 0.0446, 0.0302, 0.0252,\n",
       "       0.0384, 0.0321, 0.0286, 0.0506, 0.0365, 0.0743, 0.0723, 0.0205,\n",
       "       0.0279, 0.1004, 0.076 , 0.0191, 0.0269, 0.0528, 0.0361, 0.0294,\n",
       "       0.0602, 0.0421, 0.0368, 0.0597, 0.0258, 0.0539, 0.0689, 0.023 ,\n",
       "       0.05  , 0.1801, 0.0726, 0.0224, 0.0318, 0.051 , 0.028 , 0.0313,\n",
       "       0.1244, 0.0621, 0.0508, 0.0942, 0.077 , 0.0747, 0.1127, 0.0799,\n",
       "       0.0912, 0.1698, 0.0911, 0.05  , 0.1001, 0.049 , 0.0347, 0.0251,\n",
       "       0.1328, 0.0812, 0.1235, 0.0955, 0.1485, 0.0946, 0.1559, 0.0929,\n",
       "       0.1354, 0.0936, 0.1809, 0.1137, 0.1718, 0.0684, 0.12  , 0.0801,\n",
       "       0.0574, 0.0528, 0.0601, 0.0403, 0.0676, 0.0763, 0.0613, 0.0456,\n",
       "       0.0588, 0.0613, 0.1312, 0.0859, 0.0948, 0.0499, 0.052 , 0.0911,\n",
       "       0.0325, 0.0321, 0.025 , 0.0217, 0.0272, 0.068 , 0.0372, 0.0287,\n",
       "       0.0296, 0.0501, 0.0738, 0.0348, 0.0293, 0.0277, 0.0336, 0.0375,\n",
       "       0.0297, 0.0363, 0.0249, 0.0299, 0.0266, 0.0953, 0.0633, 0.0312,\n",
       "       0.0307, 0.0476, 0.0782, 0.0227, 0.024 , 0.0336, 0.0265, 0.023 ,\n",
       "       0.035 , 0.0365, 0.0427, 0.0366, 0.0246, 0.0864, 0.0795, 0.0218,\n",
       "       0.0382, 0.0744, 0.0657, 0.0357, 0.0228, 0.0661, 0.0471, 0.0146,\n",
       "       0.0397, 0.0349, 0.0395, 0.0464, 0.0203, 0.0447, 0.0729, 0.0205,\n",
       "       0.0433, 0.075 , 0.083 , 0.0212, 0.04  , 0.0859, 0.0581, 0.0338,\n",
       "       0.0453, 0.0414, 0.0623, 0.0866, 0.0264, 0.0572, 0.0933, 0.0392,\n",
       "       0.0314, 0.109 , 0.092 , 0.0258, 0.0393, 0.0864, 0.0511, 0.037 ,\n",
       "       0.0811, 0.074 , 0.0903, 0.1156, 0.092 , 0.1179, 0.1505, 0.1201,\n",
       "       0.1061, 0.1405, 0.1053, 0.0583, 0.0773, 0.0845, 0.0484, 0.026 ,\n",
       "       0.2203, 0.1299, 0.1846, 0.0925, 0.2102, 0.1497, 0.2352, 0.0999,\n",
       "       0.188 , 0.1712, 0.2815, 0.1449, 0.1963, 0.1609, 0.1918, 0.0808,\n",
       "       0.0825, 0.0618, 0.0858, 0.0531, 0.0997, 0.0804, 0.0711, 0.0411,\n",
       "       0.0658, 0.1118, 0.1539, 0.1242, 0.096 , 0.1132, 0.0893, 0.122 ,\n",
       "       0.0253, 0.0549, 0.0244, 0.0252, 0.0308, 0.0559, 0.0656, 0.0236,\n",
       "       0.0404, 0.0322, 0.0613, 0.0598, 0.0347, 0.0375, 0.039 , 0.0451,\n",
       "       0.0408, 0.0565, 0.0381, 0.0251, 0.033 , 0.1151, 0.089 , 0.0261,\n",
       "       0.0415, 0.0602, 0.0762, 0.0268, 0.032 , 0.0312, 0.0475, 0.0221,\n",
       "       0.0496, 0.0325, 0.0585, 0.0332, 0.0408, 0.0616, 0.0698, 0.0395,\n",
       "       0.0429, 0.0651, 0.082 , 0.0386, 0.0405, 0.0342, 0.0579, 0.0271,\n",
       "       0.0939, 0.0647, 0.0884, 0.0954, 0.14  , 0.1124, 0.1252, 0.105 ,\n",
       "       0.1049, 0.1092, 0.0498, 0.0511, 0.1082, 0.1031, 0.0545, 0.0474,\n",
       "       0.2339, 0.1581, 0.1452, 0.1777, 0.2803, 0.2541, 0.1426, 0.1622,\n",
       "       0.2905, 0.2539, 0.141 , 0.1648, 0.2805, 0.2083, 0.1199, 0.1783,\n",
       "       0.0945, 0.072 , 0.0748, 0.0624, 0.077 , 0.116 , 0.0494, 0.0385,\n",
       "       0.1258, 0.0837, 0.095 , 0.0853, 0.1383, 0.0923, 0.0896, 0.1217,\n",
       "       0.0219, 0.0325, 0.0252, 0.0218, 0.0134, 0.0766, 0.057 , 0.0276,\n",
       "       0.024 , 0.0582, 0.0787, 0.0289, 0.021 , 0.0314, 0.028 , 0.0213,\n",
       "       0.0291, 0.0395, 0.0382, 0.0296, 0.0175, 0.0512, 0.0698, 0.0267,\n",
       "       0.0245, 0.0752, 0.0793, 0.0232, 0.0263, 0.0335, 0.0342, 0.0296,\n",
       "       0.0329, 0.0374, 0.029 , 0.0595, 0.0235, 0.043 , 0.0518, 0.0456,\n",
       "       0.0474, 0.111 , 0.0492, 0.0248, 0.0317, 0.0458, 0.0297, 0.0271,\n",
       "       0.0739, 0.0451, 0.0458, 0.0813, 0.0789, 0.0538, 0.0909, 0.0962,\n",
       "       0.0869, 0.1304, 0.0801, 0.0452, 0.0858, 0.0506, 0.0496, 0.0315,\n",
       "       0.0812, 0.0385, 0.0875, 0.0608, 0.131 , 0.0597, 0.1399, 0.0882,\n",
       "       0.1321, 0.0864, 0.1482, 0.0969, 0.1444, 0.0704, 0.0887, 0.0552,\n",
       "       0.0418, 0.0448, 0.0561, 0.0491, 0.0658, 0.0617, 0.057 , 0.0653,\n",
       "       0.0605, 0.0585, 0.0923, 0.0708, 0.0723, 0.0571, 0.0414, 0.0629,\n",
       "       0.0273, 0.0428, 0.0371, 0.0306, 0.0345, 0.0721, 0.0302, 0.0345,\n",
       "       0.0224, 0.052 , 0.0572, 0.0428, 0.0288, 0.0317, 0.0263, 0.0414,\n",
       "       0.0285, 0.0348, 0.0291, 0.0282, 0.0206, 0.0705, 0.049 , 0.0296,\n",
       "       0.0214, 0.0694, 0.0656, 0.0416, 0.0306, 0.0256, 0.0202, 0.0232,\n",
       "       0.0167, 0.0198, 0.0256, 0.0207, 0.0189, 0.0481, 0.0428, 0.0321,\n",
       "       0.0217, 0.0511, 0.0531, 0.0196, 0.0206, 0.0283, 0.0257, 0.0161,\n",
       "       0.0169, 0.0269, 0.0302, 0.0294, 0.0173, 0.0262, 0.0316, 0.0299,\n",
       "       0.019 , 0.0491, 0.0637, 0.0251, 0.0201, 0.0426, 0.0338, 0.0233,\n",
       "       0.0246, 0.0225, 0.0354, 0.0744, 0.0202, 0.0372, 0.049 , 0.039 ,\n",
       "       0.0322, 0.0636, 0.0502, 0.0283, 0.0233, 0.0521, 0.0461, 0.0285,\n",
       "       0.0469, 0.0615, 0.0489, 0.0993, 0.0668, 0.0872, 0.1215, 0.0866,\n",
       "       0.0879, 0.0996, 0.0837, 0.0378, 0.055 , 0.0551, 0.041 , 0.0295,\n",
       "       0.1195, 0.0654, 0.1101, 0.0597, 0.1516, 0.0881, 0.1781, 0.0614,\n",
       "       0.1401, 0.109 , 0.2157, 0.0452, 0.1375, 0.085 , 0.1214, 0.035 ,\n",
       "       0.0435, 0.0465, 0.0585, 0.0344, 0.0746, 0.0636, 0.0506, 0.0325,\n",
       "       0.0465, 0.1114, 0.134 , 0.0451, 0.0784, 0.0642, 0.0518, 0.0704,\n",
       "       0.0234, 0.0486, 0.038 , 0.0358, 0.0266, 0.0543, 0.0312, 0.0305,\n",
       "       0.022 , 0.0611, 0.0565, 0.0329, 0.0256, 0.0419, 0.0432, 0.052 ,\n",
       "       0.023 , 0.0339, 0.0296, 0.0324, 0.0156, 0.0747, 0.0633, 0.0197,\n",
       "       0.0167, 0.0449, 0.058 , 0.0261, 0.0237, 0.0272, 0.0337, 0.0182,\n",
       "       0.024 , 0.0259, 0.0207, 0.0165, 0.0301, 0.047 , 0.0439, 0.0349,\n",
       "       0.0306, 0.0499, 0.0622, 0.0369, 0.0214, 0.0234, 0.0291, 0.0225,\n",
       "       0.0373, 0.0298, 0.0386, 0.0449, 0.0865, 0.0706, 0.0773, 0.0548,\n",
       "       0.0537, 0.0741, 0.0339, 0.0295, 0.0573, 0.0492, 0.0204, 0.0413,\n",
       "       0.0796, 0.0449, 0.0652, 0.0425, 0.1487, 0.1466, 0.0773, 0.0685,\n",
       "       0.1442, 0.1594, 0.0879, 0.0544, 0.1113, 0.0798, 0.0579, 0.0586,\n",
       "       0.0448, 0.0493, 0.0358, 0.0171, 0.0489, 0.0726, 0.0282, 0.0228,\n",
       "       0.0583, 0.0614, 0.0752, 0.0312, 0.0539, 0.0429, 0.0458, 0.0461],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = len(clusters)\n",
    "query_size = 1000\n",
    "min_threshold = 0.0\n",
    "max_threshold = 0.1\n",
    "slot = 0.001\n",
    "queries_dimension = 960\n",
    "normalize_factor = 1.0\n",
    "hidden_num = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_total_level = [[[] for _ in range(query_size)] for _ in range(cluster_size)]\n",
    "for clus in range(cluster_size):\n",
    "    for t in ground_truth_total[clus]:\n",
    "        ground_truth_total_level[t[0]][t[1]].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for cluster in clusters:\n",
    "    centroids.append(np.mean(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def prepare_for_cluster():\n",
    "    batch_size = 128\n",
    "    mini = 9999999999\n",
    "    maxi = 0\n",
    "#     train_queries = []\n",
    "#     train_thresholds = []\n",
    "#     train_targets = []\n",
    "#     train_masks = []\n",
    "#     for train_id in range(200):\n",
    "#         join_size = int(random.random() * 99 + 1)\n",
    "#         query_ids = np.random.choice(int(query_size * 0.8), join_size, replace=False)\n",
    "#         query_ids = np.pad(query_ids, (0, 100-join_size), mode='constant')\n",
    "# #         print (query_ids.size)\n",
    "#         train_mask = [[1] for _ in range(join_size)] + [[0] for _ in range(100-join_size)]\n",
    "# #         queries = []\n",
    "# #         for query_id in query_ids:\n",
    "# #             queries.append(data_test[query_id]/255.0)\n",
    "#         cardinality = 0\n",
    "#         for threshold_id, threshold in enumerate(np.arange(min_threshold, max_threshold, slot)):\n",
    "#             for query_id_idx in range(join_size):\n",
    "#                 for cluster_id in range(cluster_size):\n",
    "#                     cardinality += ground_truth_total_level[cluster_id][query_ids[query_id_idx]][threshold_id][-1]\n",
    "#             train_queries.append(query_ids)\n",
    "#             train_thresholds.append([threshold+slot])\n",
    "#             train_targets.append([cardinality])\n",
    "#             train_masks.append(train_mask)\n",
    "#             log_c = np.log(cardinality)\n",
    "#             if log_c < mini:\n",
    "#                 mini = log_c\n",
    "#             if log_c > maxi:\n",
    "#                 maxi = log_c\n",
    "\n",
    "    test_queries = []\n",
    "    test_thresholds = []\n",
    "    test_targets = []\n",
    "    test_masks = []\n",
    "    for test_id in range(50):\n",
    "        join_size = int(random.random() * 99 + 1)\n",
    "        query_ids = np.random.choice(int(query_size * 0.2), join_size, replace=False) + int(query_size * 0.8)\n",
    "        query_ids = np.pad(query_ids, (0, 100-join_size), mode='constant')\n",
    "        test_mask = [[1] for _ in range(join_size)] + [[0] for _ in range(100-join_size)]\n",
    "        cardinality = 0\n",
    "        for threshold_id, threshold in enumerate(np.arange(min_threshold, max_threshold, slot)):\n",
    "            for query_id_idx in range(join_size):\n",
    "                for cluster_id in range(cluster_size):\n",
    "                    cardinality += ground_truth_total_level[cluster_id][query_ids[query_id_idx]][threshold_id][-1]\n",
    "            test_queries.append(query_ids)\n",
    "            test_thresholds.append([threshold+slot])\n",
    "            test_targets.append([cardinality])\n",
    "            test_masks.append(test_mask)\n",
    "            log_c = np.log(cardinality)\n",
    "            if log_c < mini:\n",
    "                mini = log_c\n",
    "            if log_c > maxi:\n",
    "                maxi = log_c\n",
    "#     print (torch.FloatTensor(train_queries).shape, torch.FloatTensor(test_queries).shape)\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         torch.utils.data.TensorDataset(torch.FloatTensor(train_masks), torch.FloatTensor(train_queries), torch.FloatTensor(train_thresholds), torch.FloatTensor(train_targets)), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(test_masks), torch.FloatTensor(test_queries), torch.FloatTensor(test_thresholds), torch.FloatTensor(test_targets)), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     return train_loader, test_loader, mini, maxi\n",
    "    return None, test_loader, 0.0, 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunji/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# train_loaders, test_loaders, minis, maxis = prepare_for_cluster()\n",
    "_, test_loaders, minis, maxis = prepare_for_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Monotonic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# queries_dimension = 200\n",
    "# hidden_num = 128\n",
    "\n",
    "class Threshold_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Threshold_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_num)\n",
    "        self.fc2 = nn.Linear(hidden_num, 1)\n",
    "    \n",
    "    def forward(self, threshold):\n",
    "        t1 = F.relu(self.fc1(threshold))\n",
    "        t2 = self.fc2(t1)\n",
    "        return t2\n",
    "\n",
    "class MLP_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, queries_dimension):\n",
    "        super(MLP_Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(queries_dimension, hidden_num)\n",
    "        self.layer2 = nn.Linear(hidden_num, hidden_num)\n",
    "        self.layer3 = nn.Linear(hidden_num, hidden_num)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        join_size = inputs.shape[1]\n",
    "        inputs = inputs.view(batch_size * join_size, -1)\n",
    "        hid = F.relu(self.layer1(inputs))\n",
    "        hid = F.relu(self.layer2(hid))\n",
    "        hid = F.relu(self.layer3(hid))\n",
    "        return hid\n",
    "    \n",
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, pool_type, pool_size):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.layer0 = nn.Linear(queries_dimension, hidden_num)\n",
    "        if pool_type == 0:\n",
    "            pool_layer = nn.MaxPool1d(kernel_size=pool_size, stride=pool_size)\n",
    "        elif pool_type == 1:\n",
    "            pool_layer = nn.AvgPool1d(kernel_size=pool_size, stride=pool_size)\n",
    "        else:\n",
    "            print ('CNN_Model Init Error, invalid pool_type {}'.format(pool_type))\n",
    "            return\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding), \n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            pool_layer)\n",
    "        input_dim = int((int((hidden_num - kernel_size + 2*(padding)) / stride) + 1) / pool_size)\n",
    "        self.layer2 = nn.Linear(input_dim * out_channel, hidden_num)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        join_size = inputs.shape[1]\n",
    "        inputs = inputs.view(batch_size * join_size, -1)\n",
    "        hid = F.relu(self.layer0(inputs))\n",
    "#         print (hid.shape)\n",
    "        hid = hid.unsqueeze(2)\n",
    "        hid = hid.permute(0,2,1)\n",
    "#         print (hid.shape)\n",
    "        hid = self.layer1(hid).view(batch_size * join_size, -1)\n",
    "#         print (hid.shape)\n",
    "        hid = F.relu(self.layer2(hid))\n",
    "#         print (hid.shape)\n",
    "        return hid\n",
    "\n",
    "class Output_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputs_dim):\n",
    "        super(Output_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_dim, hidden_num)\n",
    "        self.fc2 = nn.Linear(hidden_num, 1)\n",
    "        \n",
    "    def forward(self, queries, threshold):\n",
    "        out1 = F.relu(self.fc1(queries))\n",
    "        out2 = out1 + threshold\n",
    "#         print ('out2: {0}, threshold: {1}'.format(out2.shape, threshold.shape))\n",
    "        out3 = self.fc2(out2)\n",
    "        return out3\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.nn1 = nn.Linear(queries_dimension+1, hidden_num)\n",
    "        self.n1 = nn.Linear(hidden_num, hidden_num)\n",
    "        self.n2 = nn.Linear(hidden_num, hidden_num)\n",
    "#         self.n3 = nn.Linear(hidden_num, hidden_num)\n",
    "#         self.n4 = nn.Linear(hidden_num, hidden_num)\n",
    "        self.nn2 = nn.Linear(hidden_num, 1)\n",
    "        \n",
    "    def forward(self, queries, threshold):\n",
    "        out1 = F.relu(self.nn1(torch.cat([queries, threshold],1)))\n",
    "        hid = out1\n",
    "        hid = F.relu(self.n1(hid))\n",
    "        hid = F.relu(self.n2(hid))\n",
    "#         hid = F.relu(self.n3(hid))\n",
    "#         hid = F.relu(self.n4(hid))\n",
    "#         hid = self.norm2(hid)\n",
    "        out2 = self.nn2(hid)\n",
    "        return out2\n",
    "\n",
    "def loss_fn(estimates, targets, mini, maxi):\n",
    "    est = unnormalize(estimates, mini, maxi)\n",
    "    print (torch.cat((est, targets), 1))\n",
    "    return F.mse_loss(est, targets)\n",
    "\n",
    "def l1_loss(estimates, targets, eps=1e-5):\n",
    "    estimates = torch.exp(estimates)\n",
    "#     torch.pow(10, estimates)\n",
    "    qerror = 0.0\n",
    "    for i in range(estimates.shape[0]):\n",
    "        if estimates[i] > targets[i] + 0.1:\n",
    "            qerror += ((estimates[i] / (targets[i] + 0.1)))\n",
    "        else:\n",
    "            qerror += (((targets[i] + 0.1) / estimates[i]))\n",
    "    return qerror / estimates.shape[0]\n",
    "\n",
    "def mse_loss(estimates, targets, eps=1e-5):\n",
    "#     print (torch.cat((estimates, targets), 1))\n",
    "    return F.mse_loss(estimates, torch.log(targets))\n",
    "\n",
    "def qerror_loss(preds, targets, mini, maxi):\n",
    "    qerror = []\n",
    "    preds = unnormal1ize_label(preds, mini, maxi)\n",
    "#     print (torch.cat((preds, targets), 1))\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i]/targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i]/(preds[i] + 0.1))\n",
    "    return torch.mean(torch.cat(qerror) ** 2)\n",
    "\n",
    "def print_loss(estimates, targets):\n",
    "    esti = torch.exp(estimates)\n",
    "#     print (torch.cat((estimates, esti, targets), 1))\n",
    "    qerror = []\n",
    "    for i in range(esti.shape[0]):\n",
    "        if esti[i] > targets[i] + 0.1:\n",
    "            qerror.append((esti[i] / (targets[i] + 0.1)).item())\n",
    "        else:\n",
    "            qerror.append(((targets[i] + 0.1) / esti[i]).item())\n",
    "    \n",
    "    return F.mse_loss(esti, targets), np.mean(qerror), np.max(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_test_baseline(mlp_model, threshold_model, output_model, test):\n",
    "#     embed_model.eval()\n",
    "    mlp_model.eval()\n",
    "    threshold_model.eval()\n",
    "    output_model.eval()\n",
    "    q_errors = []\n",
    "    for batch_idx, (masks, queries, thresholds, targets) in enumerate(test):\n",
    "        queries = torch.FloatTensor(data_test[queries.type(torch.LongTensor)].astype(float) / normalize_factor)\n",
    "        queries = Variable(queries)\n",
    "        thresholds = Variable(thresholds)\n",
    "        targets = Variable(targets)\n",
    "#         queries = embed_model(queries, masks)\n",
    "        batch_size = queries.shape[0]\n",
    "        join_size = queries.shape[1]\n",
    "        \n",
    "        queries = mlp_model(queries)\n",
    "        queries = (queries.view(batch_size, join_size, -1) * masks).sum(dim=1) / 100.0\n",
    "    \n",
    "        threshold = threshold_model(thresholds)\n",
    "        estimates = output_model(queries, threshold)\n",
    "\n",
    "        loss = l1_loss(estimates, targets)\n",
    "        \n",
    "        esti = torch.exp(estimates)\n",
    "        for i in range(esti.shape[0]):\n",
    "            if esti[i] > targets[i] + 0.1:\n",
    "                q_errors.append((esti[i] / (targets[i] + 0.1)).item())\n",
    "            else:\n",
    "                q_errors.append(((targets[i] + 0.1) / esti[i]).item())\n",
    "    mean = np.mean(q_errors)\n",
    "    percent90 = np.percentile(q_errors, 90)\n",
    "    percent95 = np.percentile(q_errors, 95)\n",
    "    percent99 = np.percentile(q_errors, 99)\n",
    "    median = np.median(q_errors)\n",
    "    maxi = np.max(q_errors)\n",
    "    print ('Testing: Mean Error {}, Median Error {}, 90 Percent {}, 95 Percent {}, 99 Percent {}, Max Percent {}'\n",
    "           .format(mean, median, percent90, percent95, percent99, maxi))\n",
    "    \n",
    "\n",
    "def train_and_test_baseline(mlp_model, threshold_model, output_model, opt, train, test, episode):\n",
    "    print ('size: {}'.format(len(train)))\n",
    "    test_errors = []\n",
    "    for e in range(episode):\n",
    "#         embed_model.train()\n",
    "        mlp_model.train()\n",
    "        threshold_model.train()\n",
    "        output_model.train()\n",
    "        for batch_idx, (masks, queries, thresholds, targets) in enumerate(train):\n",
    "#             print (e, batch_idx)\n",
    "#             print (queries.shape)\n",
    "    #         print (torch.cat((queries, thresholds), 1)[0])\n",
    "            queries = torch.FloatTensor(data_test[queries.type(torch.LongTensor)].astype(float) / normalize_factor)\n",
    "            queries = Variable(queries)\n",
    "            thresholds = Variable(thresholds)\n",
    "            targets = Variable(targets)\n",
    "    #         print (targets)\n",
    "            opt.zero_grad()\n",
    "#             queries = embed_model(queries, masks)\n",
    "#             print (queries.shape)\n",
    "            batch_size = queries.shape[0]\n",
    "            join_size = queries.shape[1]\n",
    "#             print (queries.shape)\n",
    "            queries = mlp_model(queries)\n",
    "#             print (masks.shape)\n",
    "            queries = (queries.view(batch_size, join_size, -1) * masks).sum(dim=1) / 100.0\n",
    "#             queries = queries.view(queries.shape[0], -1)\n",
    "#             print (queries.shape)\n",
    "            threshold = threshold_model(thresholds)\n",
    "            estimates = output_model(queries, threshold)\n",
    "            \n",
    "            loss = l1_loss(estimates, targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "#             for p in model.parameters():\n",
    "#                 p.data.clamp_(-10, 10)\n",
    "            next(threshold_model.fc1.parameters()).data.clamp_(0)\n",
    "            next(threshold_model.fc2.parameters()).data.clamp_(0)\n",
    "#             next(output_model.fc1.parameters()).data.clamp_(0)\n",
    "            next(output_model.fc2.parameters()).data.clamp_(0)\n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 print('Training: Iteration {0}, Batch {1}, Loss {2}'.format(e, batch_idx, loss.item()))\n",
    "#                 print(cnn_models[0].layer[0].weight.grad)\n",
    "        \n",
    "#         embed_model.eval()\n",
    "        mlp_model.eval()\n",
    "        threshold_model.eval()\n",
    "        output_model.eval()\n",
    "        test_loss = 0.0\n",
    "        mse_error = 0.0\n",
    "        q_mean = 0.0\n",
    "        q_max = 0.0\n",
    "        for batch_idx, (masks, queries, thresholds, targets) in enumerate(test):\n",
    "            queries = torch.FloatTensor(data_test[queries.type(torch.LongTensor)].astype(float) / normalize_factor)\n",
    "            queries = Variable(queries)\n",
    "            thresholds = Variable(thresholds)\n",
    "            targets = Variable(targets)\n",
    "#             queries = embed_model(queries, masks)\n",
    "            \n",
    "            batch_size = queries.shape[0]\n",
    "            join_size = queries.shape[1]\n",
    "            queries = mlp_model(queries)\n",
    "            queries = (queries.view(batch_size, join_size, -1) * masks).sum(dim=1) / 100.0\n",
    "#             queries = queries.view(queries.shape[0], -1)\n",
    "            threshold = threshold_model(thresholds)\n",
    "            estimates = output_model(queries, threshold)\n",
    "            \n",
    "            loss = l1_loss(estimates, targets)\n",
    "            mse, qer_mean, qer_max = print_loss(estimates, targets)\n",
    "            test_loss += loss.item()\n",
    "            mse_error += mse.item()\n",
    "            q_mean += qer_mean\n",
    "            if qer_max > q_max:\n",
    "                q_max = qer_max\n",
    "        test_loss /= len(test)\n",
    "        mse_error /= len(test)\n",
    "        q_mean /= len(test)\n",
    "        test_errors.append(q_mean)\n",
    "        print ('Testing: Iteration {0}, Loss {1}, MSE_error {2}, Q_error_mean {3}, Q_error_max {4}'.format(e, test_loss, mse_error, q_mean, q_max))\n",
    "    return np.mean(test_errors[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunji/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Mean Error 57.383323015594485, Median Error 24.494346618652344, 90 Percent 123.30300674438502, 95 Percent 198.92550430297868, 99 Percent 521.6530297851565, Max Percent 2965.298095703125\n",
      "Time used: 144.89189299999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunji/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "# embed_models = [Set_CNN(queries_dimension) for _ in range(cluster_size)]\n",
    "import time\n",
    "\n",
    "# output_models = Output_Model(hidden_num)\n",
    "# mlp_models = MLP_Model(queries_dimension)\n",
    "# mlp_models = CNN_Model(1, 2, 3, 1, 2, 1, 3)\n",
    "# threshold_models = Threshold_Model()\n",
    "\n",
    "test = test_loaders\n",
    "train = train_loaders\n",
    "paras = list()\n",
    "paras += list(mlp_models.parameters())\n",
    "paras += list(threshold_models.parameters())\n",
    "paras += list(output_models.parameters())\n",
    "opt = optim.Adam(paras, lr=0.001)\n",
    "episode = 5\n",
    "# train_and_test_baseline(mlp_models, threshold_models, output_models, opt, train, test, episode)\n",
    "start = time.clock()\n",
    "only_test_baseline(mlp_models, threshold_models, output_models, test)\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
