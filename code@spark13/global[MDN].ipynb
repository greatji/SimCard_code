{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data = h5py.File('fashion-mnist-784-euclidean.hdf5', 'r')\n",
    "dataset = np.array(data['train'])\n",
    "queries = np.array(data['test'])\n",
    "with open('clusters_fashion_mnist_784_euclidean.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)\n",
    "with open('ground_truth_fashion_mnist_784_normalized_euclidean_0_0_0_5.pkl', 'rb') as f:\n",
    "    ground_truth_total = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_total_level = [[[] for _ in range(10000)] for _ in range(100)]\n",
    "for clus in range(100):\n",
    "    for t in ground_truth_total[clus]:\n",
    "        ground_truth_total_level[t[0]][t[1]].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for cluster in clusters:\n",
    "    centroids.append(np.mean(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_normalized(x1, x2=None, eps=1e-8):\n",
    "    if np.isnan(x2):\n",
    "        return 1.0\n",
    "    left = x1 / 255.0\n",
    "    right = x2 / 255.0\n",
    "    return np.sqrt(((left - right) ** 2).mean())\n",
    "\n",
    "train_features = []\n",
    "train_thresholds = []\n",
    "train_targets = []\n",
    "slot = 0.01\n",
    "for query_id in range(8000):\n",
    "    cardinality = [0 for _ in range(100)]\n",
    "    distances2centroids = []\n",
    "    for cc in centroids:\n",
    "        distances2centroids.append(euclidean_dist_normalized(queries[query_id], cc))\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.0, 0.5, slot)):\n",
    "        indicator = []\n",
    "        for cluster_id in range(100):\n",
    "            cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "        feature = queries[query_id] / 255.0\n",
    "        train_features.append(feature)\n",
    "        train_thresholds.append([threshold+slot])\n",
    "        train_targets.append(indicator)\n",
    "                \n",
    "test_features = []\n",
    "test_thresholds = []\n",
    "test_targets = []\n",
    "slot = 0.01\n",
    "for query_id in range(8000,10000):\n",
    "    cardinality = [0 for _ in range(100)]\n",
    "    distances2centroids = []\n",
    "    for cc in centroids:d\n",
    "        distances2centroids.append(euclidean_dist_normalized(queries[query_id], cc))\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.0, 0.5, slot)):\n",
    "        indicator = []\n",
    "        for cluster_id in range(100):\n",
    "            cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "        feature = queries[query_id] / 255.0\n",
    "        test_features.append(feature)\n",
    "        test_thresholds.append([threshold+slot])\n",
    "        test_targets.append(indicator)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(train_features), torch.FloatTensor(train_thresholds), torch.FloatTensor(train_targets)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(test_features), torch.FloatTensor(test_thresholds), torch.FloatTensor(test_targets)), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Density Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# dimensionality of hidden layer\n",
    "h = 64\n",
    "# K mixing components (PRML p. 274)\n",
    "# Can also formulate as a K-dimensional, one-hot\n",
    "# encoded, latent variable $z$, and have the model\n",
    "# produce values for $\\mu_k = p(z_k = 1)$, i.e., the\n",
    "# prob of each possible state of $z$. (PRML p. 430)\n",
    "k = 10  # 3\n",
    "# We specialize to the case of isotropic covariances (PRML p. 273),\n",
    "# so the covariance matrix is diagonal with equal diagonal elements,\n",
    "# i.e., the variances for each dimension of y are equivalent.\n",
    "# therefore, the MDN outputs pi & sigma scalars for each mixture\n",
    "# component, and a mu vector for each mixture component containing\n",
    "# means for each target variable.\n",
    "# NOTE: we could use the shorthand `d_out = 3*k`, since our target\n",
    "# variable for this project only has a dimensionality of 1, but\n",
    "# the following is more general.\n",
    "# d_out = (t + 2) * k  # t is L from PRML p. 274\n",
    "# NOTE: actually cleaner to just separate pi, sigma^2, & mu into\n",
    "# separate functions.\n",
    "t = 1\n",
    "d_pi = k\n",
    "d_sigmasq = k\n",
    "d_mu = t * k\n",
    "\n",
    "d = 784\n",
    "d_out = 100\n",
    "# n = len(training_features)\n",
    "# dimention of input features\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.nn1 = nn.Linear(d+1, h)\n",
    "        self.nn2 = nn.Linear(h, h)\n",
    "        self.nn_pi = nn.Linear(h, d_pi)\n",
    "        self.nn_sigmasq = nn.Linear(h, d_sigmasq)\n",
    "        self.nn_mu = nn.Linear(h, d_mu)\n",
    "#         self.nn_threshold_1 = nn.Linear(1, h)\n",
    "#         self.nn_threshold_2 = nn.Linear(h, 1)\n",
    "        \n",
    "#         self.w1 = Variable(torch.randn(d, h) * np.sqrt(2/(d+h)), requires_grad=True)\n",
    "#         self.b1 = Variable(torch.zeros(1, h), requires_grad=True)\n",
    "#         self.w_pi = Variable(torch.randn(h, d_pi) * np.sqrt(2/(d+h)), requires_grad=True)\n",
    "#         self.b_pi = Variable(torch.zeros(1, d_pi), requires_grad=True)\n",
    "#         self.w_sigmasq = Variable(torch.randn(h, d_sigmasq) * np.sqrt(2/(d+h)), requires_grad=True)\n",
    "#         self.b_sigmasq = Variable(torch.zeros(1, d_sigmasq), requires_grad=True)\n",
    "#         self.w_mu = Variable(torch.randn(h, d_mu) * np.sqrt(2/(d+h)), requires_grad=True)\n",
    "#         self.b_mu = Variable(torch.zeros(1, d_mu), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, threshold):\n",
    "#         print ('input: ', x)\n",
    "        out = F.leaky_relu(self.nn1(torch.cat((x, threshold), dim=1)))  # shape (n, h)\n",
    "        out = F.leaky_relu(self.nn2(out))  # shape (n, h)\n",
    "#         print ('out: ', out)\n",
    "        #out = F.leaky_relu(x.mm(w1) + b1)  # interesting possibility\n",
    "        pi = F.softmax(self.nn_pi(out), dim=1)  # p(z_k = 1) for all k; K mixing components that sum to 1; shape (n, k)\n",
    "        sigmasq = torch.exp(self.nn_sigmasq(out))  # K gaussian variances, which must be >= 0; shape (n, k)\n",
    "#         sigmasq = self.nn_sigmasq(out)\n",
    "        mu = torch.exp(self.nn_mu(out))  # K * L gaussian means; shape (n, k*t)\n",
    "#         print ('pi: ', pi)\n",
    "#         print ('sigmasq: ', sigmasq)\n",
    "#         print ('mu: ', mu)\n",
    "        outputs_y = torch.zeros(x.size()[0], d_out)  # p(y|x)\n",
    "        outputs_x = Variable( torch.FloatTensor([[e for e in range(d_out)] for _ in range(x.size()[0])]))\n",
    "        for i in range(k):  # marginalize over z\n",
    "            likelihood_z_x = gaussian_pdf(outputs_x, mu[:, i*t:(i+1)*t], sigmasq[:, i:(i+1)])\n",
    "#             print (likelihood_z_x)\n",
    "            prior_z = pi[:, i:(i+1)]\n",
    "#             print (prior_z.shape)\n",
    "    #         print ('likelihood_z_x: ', likelihood_z_x)\n",
    "#             outputs_y += prior_z * likelihood_z_x\n",
    "            outputs_y += likelihood_z_x\n",
    "#         hid_threshold_1 = F.relu(self.nn_threshold_1(threshold))\n",
    "#         hid_threshold_2 = F.relu(self.nn_threshold_2(hid_threshold_1))\n",
    "#         return F.sigmoid(outputs_y - threshold)\n",
    "#         print ('outputs_y: ', outputs_y)\n",
    "#         print ('threshold: ', threshold)\n",
    "        return F.sigmoid(outputs_y - threshold)\n",
    "#         return outputs_y\n",
    "\n",
    "\n",
    "def gaussian_pdf(x, mu, sigmasq):\n",
    "    # NOTE: we could use the new `torch.distributions` package for this now\n",
    "#     print (x.shape, mu.shape, sigmasq.shape)\n",
    "#     return (1/torch.sqrt(2*np.pi*sigmasq)) * torch.exp((-1/(2*sigmasq)) * torch.norm((x-mu), 2, 1)**2)\n",
    "    return (1/torch.sqrt(2*np.pi*sigmasq)) * torch.exp((-1/(2*sigmasq)) * (x-mu)**2)   \n",
    "\n",
    "def loss_fn(estimates, targets):\n",
    "    return F.mse_loss(estimates, targets)\n",
    "\n",
    "def print_loss(estimates, targets):\n",
    "    true_positive = 0.0\n",
    "    true_negative = 0.0\n",
    "    false_positive = 0.0\n",
    "    false_negative = 0.0\n",
    "    num_elements = estimates.shape[1]\n",
    "    for est, tar in zip(estimates, targets):\n",
    "        for i in range(num_elements):\n",
    "            if est[i] < 0.5 and tar[i] == 0:\n",
    "                true_negative += 1\n",
    "            elif est[i] < 0.5 and tar[i] == 1:\n",
    "                false_positive += 1\n",
    "            elif est[i] >= 0.5 and tar[i] == 0:\n",
    "                false_negative += 1\n",
    "            else:\n",
    "                true_positive += 1\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Iteration 0, Batch 0, Loss 0.27266162633895874\n",
      "Training: Iteration 0, Batch 100, Loss 0.26595133543014526\n",
      "Training: Iteration 0, Batch 200, Loss 0.2623739242553711\n",
      "Training: Iteration 0, Batch 300, Loss 0.25652915239334106\n",
      "Training: Iteration 0, Batch 400, Loss 0.2568981349468231\n",
      "Training: Iteration 0, Batch 500, Loss 0.2606426179409027\n",
      "Training: Iteration 0, Batch 600, Loss 0.2609802186489105\n",
      "Training: Iteration 0, Batch 700, Loss 0.2599741220474243\n",
      "Training: Iteration 0, Batch 800, Loss 0.2596185505390167\n",
      "Training: Iteration 0, Batch 900, Loss 0.2558780610561371\n",
      "Training: Iteration 0, Batch 1000, Loss 0.257240355014801\n",
      "Training: Iteration 0, Batch 1100, Loss 0.2593942880630493\n",
      "Training: Iteration 0, Batch 1200, Loss 0.26098722219467163\n",
      "Training: Iteration 0, Batch 1300, Loss 0.2582543194293976\n",
      "Training: Iteration 0, Batch 1400, Loss 0.2639160454273224\n",
      "Training: Iteration 0, Batch 1500, Loss 0.26550450921058655\n",
      "Training: Iteration 0, Batch 1600, Loss 0.2590753436088562\n",
      "Training: Iteration 0, Batch 1700, Loss 0.2658197581768036\n",
      "Training: Iteration 0, Batch 1800, Loss 0.25779983401298523\n",
      "Training: Iteration 0, Batch 1900, Loss 0.25637561082839966\n",
      "Training: Iteration 0, Batch 2000, Loss 0.2667783200740814\n",
      "Training: Iteration 0, Batch 2100, Loss 0.257947713136673\n",
      "Training: Iteration 0, Batch 2200, Loss 0.2597058415412903\n",
      "Training: Iteration 0, Batch 2300, Loss 0.26157546043395996\n",
      "Training: Iteration 0, Batch 2400, Loss 0.2590898871421814\n",
      "Training: Iteration 0, Batch 2500, Loss 0.2605309784412384\n",
      "Training: Iteration 0, Batch 2600, Loss 0.25899577140808105\n",
      "Training: Iteration 0, Batch 2700, Loss 0.25195997953414917\n",
      "Training: Iteration 0, Batch 2800, Loss 0.26673609018325806\n",
      "Training: Iteration 0, Batch 2900, Loss 0.2658616900444031\n",
      "Training: Iteration 0, Batch 3000, Loss 0.26699206233024597\n",
      "Training: Iteration 0, Batch 3100, Loss 0.2673868238925934\n",
      "Testing: Iteration 0, Batch 0, Loss 0.26291415095329285, Precision 0.012189292543021032, Recall 0.3\n",
      "Testing: Iteration 0, Batch 100, Loss 0.25641196966171265, Precision 0.017964071856287425, Recall 0.3988439306358382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-ce34190fd3d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-56cc527ef21c>\u001b[0m in \u001b[0;36mprint_loss\u001b[0;34m(estimates, targets)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mtrue_negative\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "opt = optim.Adam(model.parameters(), lr=0.0001)\n",
    "for e in range(1):\n",
    "    model.train()\n",
    "    for batch_idx, (features, thresholds, targets) in enumerate(train_loader):\n",
    "        x = Variable(features)\n",
    "        z = Variable(thresholds)\n",
    "        opt.zero_grad()\n",
    "        estimates = model(x, z)\n",
    "#         print ('estimates: ', estimates)\n",
    "#         print ('targets: ', targets)\n",
    "        loss = loss_fn(estimates, targets)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Training: Iteration {0}, Batch {1}, Loss {2}'.format(e, batch_idx, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    for batch_idx, (features, thresholds, targets) in enumerate(test_loader):\n",
    "        x = Variable(features)\n",
    "        z = Variable(thresholds)\n",
    "        estimates = model(x, z)\n",
    "        loss = loss_fn(estimates, targets)\n",
    "        test_loss += loss.item()\n",
    "        prec, rec = print_loss(estimates, targets)\n",
    "        precision += prec\n",
    "        recall += rec\n",
    "        if batch_idx % 100 == 0:\n",
    "            print ('Testing: Iteration {0}, Batch {1}, Loss {2}, Precision {3}, Recall {4}'.format(e, batch_idx, loss.item(), prec, rec))\n",
    "    test_loss /= len(test_loader)\n",
    "    precision /= len(test_loader)\n",
    "    recall /= len(test_loader)\n",
    "    print ('Testing: Loss {0}, Precision {1}, Recall {2}'.format(test_loss, precision, recall))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmasq = torch.exp(model.nn_sigmasq(F.leaky_relu(model.nn2(F.leaky_relu(model.nn1(x))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.exp(model.nn_mu(F.leaky_relu(model.nn2(F.leaky_relu(model.nn1(x))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_x = torch.FloatTensor([[e for e in range(d_out)] for _ in range(x.size()[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7365e+01, 1.5958e-04, 6.1235e-05,  ..., 2.1868e-04, 2.2945e+03,\n",
       "         3.7754e-04],\n",
       "        [1.0974e+01, 2.3538e-03, 1.1906e-03,  ..., 2.8051e-03, 2.3124e+02,\n",
       "         4.1515e-03],\n",
       "        [2.2014e+00, 1.2320e-01, 9.9651e-02,  ..., 1.3594e-01, 5.7992e+00,\n",
       "         1.6005e-01],\n",
       "        ...,\n",
       "        [1.6128e+00, 2.7502e-01, 2.4669e-01,  ..., 3.1526e-01, 2.8909e+00,\n",
       "         3.1943e-01],\n",
       "        [1.1359e+02, 5.7741e-06, 1.4284e-06,  ..., 8.5517e-06, 5.6936e+04,\n",
       "         1.7601e-05],\n",
       "        [7.1575e+00, 3.2246e-03, 1.7251e-03,  ..., 3.0635e-03, 1.3437e+02,\n",
       "         4.9532e-03]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.7011e-04, 3.3179e-01, 5.1786e+00,  ..., 8.8829e+00, 5.1800e-03,\n",
       "         2.1166e-01],\n",
       "        [8.0833e-03, 4.9907e-01, 3.5145e+00,  ..., 5.3116e+00, 2.6121e-02,\n",
       "         3.5835e-01],\n",
       "        [2.0573e-01, 7.0084e-01, 1.4859e+00,  ..., 1.5829e+00, 2.5451e-01,\n",
       "         7.5853e-01],\n",
       "        ...,\n",
       "        [3.9598e-01, 7.6149e-01, 1.2221e+00,  ..., 1.2408e+00, 4.3881e-01,\n",
       "         8.5934e-01],\n",
       "        [6.4007e-05, 2.3189e-01, 1.0889e+01,  ..., 2.2670e+01, 7.5969e-04,\n",
       "         1.1293e-01],\n",
       "        [8.5521e-03, 4.5572e-01, 3.2901e+00,  ..., 4.7649e+00, 2.0697e-02,\n",
       "         3.7116e-01]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
       "        [ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
       "        [ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
       "        ...,\n",
       "        [ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
       "        [ 0.,  1.,  2.,  ..., 97., 98., 99.],\n",
       "        [ 0.,  1.,  2.,  ..., 97., 98., 99.]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_pdf(outputs_x, mu[:,0:1], sigmasq[:, 0:1])[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8654e-02],\n",
       "        [3.5490e-02],\n",
       "        [3.8888e-02],\n",
       "        [4.1196e-03],\n",
       "        [3.2926e-04],\n",
       "        [1.0046e-02],\n",
       "        [1.0499e-02],\n",
       "        [2.8966e-03],\n",
       "        [7.8504e-05],\n",
       "        [4.5916e-06],\n",
       "        [1.1338e-03],\n",
       "        [2.0935e-02],\n",
       "        [3.9623e-05],\n",
       "        [4.0923e-02],\n",
       "        [2.6231e-03],\n",
       "        [1.2288e-04],\n",
       "        [1.9012e-03],\n",
       "        [7.5938e-04],\n",
       "        [3.9229e-02],\n",
       "        [8.3168e-04],\n",
       "        [2.4525e-04],\n",
       "        [2.0062e-04],\n",
       "        [1.2317e-04],\n",
       "        [2.2823e-03],\n",
       "        [5.6996e-02],\n",
       "        [6.4427e-03],\n",
       "        [4.5723e-04],\n",
       "        [1.1053e-02],\n",
       "        [9.6249e-03],\n",
       "        [1.3122e-03],\n",
       "        [1.9814e-05],\n",
       "        [1.6714e-04],\n",
       "        [6.7120e-03],\n",
       "        [1.0815e-01],\n",
       "        [5.6914e-04],\n",
       "        [2.6231e-03],\n",
       "        [5.8650e-02],\n",
       "        [4.1184e-02],\n",
       "        [8.4417e-04],\n",
       "        [3.0058e-04],\n",
       "        [1.3660e-02],\n",
       "        [3.2268e-03],\n",
       "        [2.0867e-04],\n",
       "        [6.4934e-04],\n",
       "        [1.4248e-03],\n",
       "        [4.1926e-02],\n",
       "        [8.5405e-04],\n",
       "        [6.5864e-03],\n",
       "        [1.4135e-02],\n",
       "        [4.7238e-02],\n",
       "        [2.6042e-04],\n",
       "        [9.7897e-02],\n",
       "        [7.5765e-04],\n",
       "        [1.1238e-03],\n",
       "        [4.2029e-04],\n",
       "        [4.9195e-02],\n",
       "        [9.6383e-03],\n",
       "        [4.0763e-04],\n",
       "        [6.1118e-05],\n",
       "        [2.2233e-02],\n",
       "        [1.7277e-04],\n",
       "        [1.6811e-03],\n",
       "        [2.8528e-02],\n",
       "        [3.7245e-02],\n",
       "        [2.9309e-03],\n",
       "        [5.1284e-03],\n",
       "        [6.6423e-03],\n",
       "        [4.0165e-02],\n",
       "        [1.6480e-02],\n",
       "        [2.7535e-05],\n",
       "        [1.4193e-03],\n",
       "        [5.9219e-04],\n",
       "        [1.0828e-03],\n",
       "        [5.6822e-03],\n",
       "        [1.7219e-03],\n",
       "        [1.6134e-04],\n",
       "        [1.0585e-05],\n",
       "        [5.8730e-03],\n",
       "        [1.2504e-04],\n",
       "        [1.0843e-01],\n",
       "        [1.6061e-02],\n",
       "        [3.7686e-04],\n",
       "        [5.9248e-03],\n",
       "        [6.1181e-02],\n",
       "        [6.9503e-03],\n",
       "        [8.7051e-02],\n",
       "        [2.7418e-02],\n",
       "        [3.7255e-03],\n",
       "        [1.6640e-03],\n",
       "        [6.2712e-02],\n",
       "        [9.5983e-02],\n",
       "        [2.3943e-04],\n",
       "        [6.5494e-02],\n",
       "        [7.1742e-02],\n",
       "        [2.6474e-04],\n",
       "        [3.2407e-02],\n",
       "        [9.7169e-02],\n",
       "        [3.7338e-02],\n",
       "        [4.4205e-03],\n",
       "        [7.7603e-03],\n",
       "        [9.8685e-02],\n",
       "        [7.3304e-05],\n",
       "        [1.9573e-03],\n",
       "        [2.8449e-03],\n",
       "        [1.2660e-02],\n",
       "        [5.1060e-03],\n",
       "        [1.2101e-03],\n",
       "        [8.3182e-02],\n",
       "        [2.4941e-04],\n",
       "        [2.7034e-04],\n",
       "        [3.2165e-04],\n",
       "        [1.0738e-01],\n",
       "        [4.9842e-02],\n",
       "        [1.4708e-03],\n",
       "        [6.6037e-03],\n",
       "        [1.7472e-04],\n",
       "        [2.9434e-02],\n",
       "        [2.3657e-02],\n",
       "        [7.6443e-03],\n",
       "        [1.0556e-01],\n",
       "        [9.9346e-03],\n",
       "        [8.9213e-05],\n",
       "        [1.1947e-04],\n",
       "        [8.6315e-02],\n",
       "        [1.0712e-03],\n",
       "        [1.7284e-04],\n",
       "        [4.7265e-02],\n",
       "        [9.2626e-02]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmasq[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.1213],\n",
       "        [ -4.8135],\n",
       "        [ -4.5288],\n",
       "        [ -8.8915],\n",
       "        [-13.2455],\n",
       "        [ -7.5993],\n",
       "        [ -6.7905],\n",
       "        [-10.3428],\n",
       "        [-15.4605],\n",
       "        [-19.9681],\n",
       "        [-10.9578],\n",
       "        [ -5.7970],\n",
       "        [-17.0164],\n",
       "        [ -4.5311],\n",
       "        [ -8.9885],\n",
       "        [-14.4900],\n",
       "        [ -9.8827],\n",
       "        [-11.7616],\n",
       "        [ -4.7673],\n",
       "        [-11.4704],\n",
       "        [-13.9492],\n",
       "        [-15.0327],\n",
       "        [-15.5136],\n",
       "        [ -9.3282],\n",
       "        [ -3.8861],\n",
       "        [ -7.9119],\n",
       "        [-13.2706],\n",
       "        [ -7.6553],\n",
       "        [ -8.1451],\n",
       "        [-10.8335],\n",
       "        [-17.9672],\n",
       "        [-15.1711],\n",
       "        [ -7.6878],\n",
       "        [ -3.1328],\n",
       "        [-12.1910],\n",
       "        [ -8.9885],\n",
       "        [ -3.6594],\n",
       "        [ -4.6261],\n",
       "        [-11.2487],\n",
       "        [-13.5288],\n",
       "        [ -7.6578],\n",
       "        [-10.2899],\n",
       "        [-14.8749],\n",
       "        [-13.0333],\n",
       "        [-10.7198],\n",
       "        [ -5.3329],\n",
       "        [-11.7752],\n",
       "        [ -8.6176],\n",
       "        [ -6.1946],\n",
       "        [ -4.3192],\n",
       "        [-13.8362],\n",
       "        [ -3.7751],\n",
       "        [-12.1066],\n",
       "        [-11.4266],\n",
       "        [-12.8993],\n",
       "        [ -5.0714],\n",
       "        [ -7.1051],\n",
       "        [-12.4295],\n",
       "        [-15.8926],\n",
       "        [ -5.6217],\n",
       "        [-13.9282],\n",
       "        [ -9.7987],\n",
       "        [ -4.6834],\n",
       "        [ -4.7505],\n",
       "        [-10.2480],\n",
       "        [ -7.8597],\n",
       "        [ -7.2904],\n",
       "        [ -4.2306],\n",
       "        [ -6.1342],\n",
       "        [-17.3796],\n",
       "        [-10.3839],\n",
       "        [-11.9189],\n",
       "        [-10.8358],\n",
       "        [ -9.3240],\n",
       "        [-10.5851],\n",
       "        [-14.0426],\n",
       "        [-18.9311],\n",
       "        [ -7.8586],\n",
       "        [-15.0419],\n",
       "        [ -3.3424],\n",
       "        [ -6.5088],\n",
       "        [-13.5725],\n",
       "        [ -8.4524],\n",
       "        [ -3.6564],\n",
       "        [ -7.6085],\n",
       "        [ -2.9195],\n",
       "        [ -4.9720],\n",
       "        [ -9.4649],\n",
       "        [-11.4425],\n",
       "        [ -4.0693],\n",
       "        [ -3.7796],\n",
       "        [-13.3937],\n",
       "        [ -3.5742],\n",
       "        [ -3.8943],\n",
       "        [-13.7217],\n",
       "        [ -5.4660],\n",
       "        [ -2.7050],\n",
       "        [ -4.3147],\n",
       "        [ -8.1455],\n",
       "        [ -7.0401],\n",
       "        [ -3.2601],\n",
       "        [-15.7054],\n",
       "        [-10.1484],\n",
       "        [ -8.8142],\n",
       "        [ -7.4064],\n",
       "        [ -9.5018],\n",
       "        [-11.4157],\n",
       "        [ -3.2739],\n",
       "        [-12.6949],\n",
       "        [-13.1131],\n",
       "        [-12.9387],\n",
       "        [ -2.5895],\n",
       "        [ -3.9027],\n",
       "        [-11.0190],\n",
       "        [ -8.4263],\n",
       "        [-13.9245],\n",
       "        [ -5.3912],\n",
       "        [ -5.8978],\n",
       "        [ -7.4606],\n",
       "        [ -2.6469],\n",
       "        [ -8.1467],\n",
       "        [-14.9564],\n",
       "        [-14.9395],\n",
       "        [ -2.9435],\n",
       "        [-11.2317],\n",
       "        [-14.0942],\n",
       "        [ -4.3493],\n",
       "        [ -2.8503]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MDN_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (nn1): Linear(in_features=201, out_features=256, bias=True)\n",
       "  (nn_pi): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (nn_sigmasq): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (nn_mu): Linear(in_features=256, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('MDN_model'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probablity_for_single(pi, sigmasq, mu, targets):\n",
    "    # rather than sample the single conditional mode at each\n",
    "    # point, we could sample many points from the GMM produced\n",
    "    # by the model for each point, yielding a dense set of\n",
    "    # predictions\n",
    "    N, K = pi.shape\n",
    "    _, KT = mu.shape\n",
    "    T = int(KT / K)\n",
    "    class_num = targets.shape[1]\n",
    "    out = torch.zeros(N, class_num)\n",
    "    print ('here')\n",
    "    for c in range(class_num):\n",
    "        print ('c: {}, k: {}'.format(c, k))\n",
    "        target = targets[:, c]\n",
    "        prob = torch.zeros(N)\n",
    "        for i in range(K):  # marginalize over z\n",
    "            likelihood_z_x = gaussian_pdf(target, mu[:, i*T:(i+1)*T], sigmasq[:, i])\n",
    "            prior_z = pi[:, i]\n",
    "            prob += (likelihood_z_x * prior_z)\n",
    "        out[:, c] += prob\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "print (torch.FloatTensor(test_navigate_input).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prepared\n",
      "here\n",
      "c: 0, k: 50\n",
      "c: 1, k: 50\n",
      "c: 2, k: 50\n",
      "c: 3, k: 50\n",
      "c: 4, k: 50\n",
      "c: 5, k: 50\n",
      "c: 6, k: 50\n",
      "c: 7, k: 50\n",
      "c: 8, k: 50\n",
      "c: 9, k: 50\n",
      "c: 10, k: 50\n",
      "c: 11, k: 50\n",
      "c: 12, k: 50\n",
      "c: 13, k: 50\n",
      "c: 14, k: 50\n",
      "c: 15, k: 50\n",
      "c: 16, k: 50\n",
      "c: 17, k: 50\n",
      "c: 18, k: 50\n",
      "c: 19, k: 50\n",
      "c: 20, k: 50\n",
      "c: 21, k: 50\n",
      "c: 22, k: 50\n",
      "c: 23, k: 50\n",
      "c: 24, k: 50\n",
      "c: 25, k: 50\n",
      "c: 26, k: 50\n",
      "c: 27, k: 50\n",
      "c: 28, k: 50\n",
      "c: 29, k: 50\n",
      "c: 30, k: 50\n",
      "c: 31, k: 50\n",
      "c: 32, k: 50\n",
      "c: 33, k: 50\n",
      "c: 34, k: 50\n",
      "c: 35, k: 50\n",
      "c: 36, k: 50\n",
      "c: 37, k: 50\n",
      "c: 38, k: 50\n",
      "c: 39, k: 50\n",
      "c: 40, k: 50\n",
      "c: 41, k: 50\n",
      "c: 42, k: 50\n",
      "c: 43, k: 50\n",
      "c: 44, k: 50\n",
      "c: 45, k: 50\n",
      "c: 46, k: 50\n",
      "c: 47, k: 50\n",
      "c: 48, k: 50\n",
      "c: 49, k: 50\n",
      "c: 50, k: 50\n",
      "c: 51, k: 50\n",
      "c: 52, k: 50\n",
      "c: 53, k: 50\n",
      "c: 54, k: 50\n",
      "c: 55, k: 50\n",
      "c: 56, k: 50\n",
      "c: 57, k: 50\n",
      "c: 58, k: 50\n",
      "c: 59, k: 50\n",
      "c: 60, k: 50\n",
      "c: 61, k: 50\n",
      "c: 62, k: 50\n",
      "c: 63, k: 50\n",
      "c: 64, k: 50\n",
      "c: 65, k: 50\n",
      "c: 66, k: 50\n",
      "c: 67, k: 50\n",
      "c: 68, k: 50\n",
      "c: 69, k: 50\n",
      "c: 70, k: 50\n",
      "c: 71, k: 50\n",
      "c: 72, k: 50\n",
      "c: 73, k: 50\n",
      "c: 74, k: 50\n",
      "c: 75, k: 50\n",
      "c: 76, k: 50\n",
      "c: 77, k: 50\n",
      "c: 78, k: 50\n",
      "c: 79, k: 50\n",
      "c: 80, k: 50\n",
      "c: 81, k: 50\n",
      "c: 82, k: 50\n",
      "c: 83, k: 50\n",
      "c: 84, k: 50\n",
      "c: 85, k: 50\n",
      "c: 86, k: 50\n",
      "c: 87, k: 50\n",
      "c: 88, k: 50\n",
      "c: 89, k: 50\n",
      "c: 90, k: 50\n",
      "c: 91, k: 50\n",
      "c: 92, k: 50\n",
      "c: 93, k: 50\n",
      "c: 94, k: 50\n",
      "c: 95, k: 50\n",
      "c: 96, k: 50\n",
      "c: 97, k: 50\n",
      "c: 98, k: 50\n",
      "c: 99, k: 50\n"
     ]
    }
   ],
   "source": [
    "def test_navigation(features):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    x = features\n",
    "    pi, sigmasq, mu = model(x)\n",
    "    targets = torch.FloatTensor([np.arange(0.0, 1.0, 0.01) for _ in range(x.shape[0])])\n",
    "    output = predict_probablity_for_single(pi, sigmasq, mu, targets)\n",
    "    return output\n",
    "\n",
    "test_navigate_input = []\n",
    "test_navigate_cards = []\n",
    "slot = 0.01\n",
    "for query_id in range(8000,8010):\n",
    "    cardinality = [0 for _ in range(100)]\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.0, 0.5, slot)):\n",
    "        for cluster_id in range(100):\n",
    "            cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "        feature = np.append(queries[query_id], [threshold+slot])\n",
    "        test_navigate_input.append(feature)\n",
    "        test_navigate_cards.append(cardinality.copy())\n",
    "print ('data prepared')\n",
    "estimate = test_navigation(torch.FloatTensor(test_navigate_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [12],\n",
      "        [16],\n",
      "        [17],\n",
      "        [18],\n",
      "        [19],\n",
      "        [20],\n",
      "        [21],\n",
      "        [22],\n",
      "        [23],\n",
      "        [24],\n",
      "        [25],\n",
      "        [26],\n",
      "        [27],\n",
      "        [28],\n",
      "        [29],\n",
      "        [30],\n",
      "        [31],\n",
      "        [32],\n",
      "        [33],\n",
      "        [34],\n",
      "        [35],\n",
      "        [36],\n",
      "        [37],\n",
      "        [38],\n",
      "        [39],\n",
      "        [40],\n",
      "        [41],\n",
      "        [42],\n",
      "        [43],\n",
      "        [44],\n",
      "        [45],\n",
      "        [46],\n",
      "        [47],\n",
      "        [48],\n",
      "        [49],\n",
      "        [50],\n",
      "        [51],\n",
      "        [52],\n",
      "        [53],\n",
      "        [54],\n",
      "        [55],\n",
      "        [56],\n",
      "        [57],\n",
      "        [58],\n",
      "        [59],\n",
      "        [60],\n",
      "        [61],\n",
      "        [62],\n",
      "        [63],\n",
      "        [64],\n",
      "        [65],\n",
      "        [67],\n",
      "        [68],\n",
      "        [69],\n",
      "        [70],\n",
      "        [71],\n",
      "        [72],\n",
      "        [73],\n",
      "        [74],\n",
      "        [75],\n",
      "        [76],\n",
      "        [77],\n",
      "        [78],\n",
      "        [79],\n",
      "        [80],\n",
      "        [81],\n",
      "        [82],\n",
      "        [83],\n",
      "        [84],\n",
      "        [86],\n",
      "        [87],\n",
      "        [90],\n",
      "        [91],\n",
      "        [92],\n",
      "        [93],\n",
      "        [94]])\n",
      "tensor([[12],\n",
      "        [13],\n",
      "        [25],\n",
      "        [37]])\n"
     ]
    }
   ],
   "source": [
    "print ((estimate[41] > 5e-32).nonzero())\n",
    "print ((torch.FloatTensor(test_navigate_cards)[41] > 2).nonzero())\n",
    "# test_navigate_cards[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#     torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_dataset = np.array(f['train'])\n",
    "test_dataset = np.array(f['test'])\n",
    "train_lefts, train_rights, test_lefts, test_rights = prepare_dataset(train_dataset, test_dataset, train_num, test_num)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    (train_lefts, train_rights), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    (test_lefts, test_rights), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_distances, input_distances = test(model, device, train_loader)\n",
    "hash_distances, input_distances = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = torch.FloatTensor([f['train'][0] for x in range(999)])\n",
    "rights = torch.FloatTensor(f['train'][1:1000])\n",
    "inputdistance = angular_distance(lefts, rights).detach().numpy()\n",
    "hashdistance = l1_distance(model(lefts), model(rights)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xx in zip(inputdistance, hashdistance):\n",
    "#     print (xx[0], xx[1])\n",
    "index_1 = np.argsort(hashdistance, 0)\n",
    "index_2 = np.argsort(inputdistance, 0)\n",
    "# np.random.shuffle(index_2)\n",
    "\n",
    "input_index = {}\n",
    "for pos, idx in enumerate(index_2):\n",
    "    input_index[idx] = pos\n",
    "sum = 0.0\n",
    "for pos, idx in enumerate(index_1):\n",
    "    sum += np.abs(pos - input_index[idx])\n",
    "sum / len(index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.sort(inputdistance, 0)\n",
    "plt.plot(xxx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "distances = []\n",
    "for i in index_1:\n",
    "    distances.append(math.floor(inputdistance[i].item()* 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(torch.FloatTensor(f['train'][0]).unsqueeze(0), torch.FloatTensor(f['train'][6]).unsqueeze(0), dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(hash_distances[0][0:30], input_distances[0][0:30]):\n",
    "    print (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vector = model(torch.FloatTensor(f['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = model(torch.FloatTensor(f['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(vector):\n",
    "    query_codes = []\n",
    "    for v in vector:\n",
    "        binary_code = []\n",
    "        for e in v:\n",
    "            if e < 0.5:\n",
    "                binary_code.append(0)\n",
    "            else:\n",
    "                binary_code.append(1)\n",
    "        query_codes.append(binary_code)\n",
    "    return np.array(query_codes)\n",
    "dataset_binary = binarization(dataset_vector.detach().numpy())\n",
    "query_binary = binarization(query_vector.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "hash_table = {}\n",
    "for idx, point in enumerate(dataset_binary):\n",
    "    pos = 0\n",
    "    key = 0\n",
    "    for d in point:\n",
    "        key += d * math.pow(2, pos)\n",
    "        pos += 1\n",
    "    if key in hash_table:\n",
    "        hash_table[key].append(idx)\n",
    "    else:\n",
    "        hash_table[key] = [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['neighbors'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidate_distance(vector, hash_table, candidate_num):\n",
    "    candidate = []\n",
    "    for point in query_binary:\n",
    "        cand = []\n",
    "        dis = 0\n",
    "        while len(cand) < 100:\n",
    "            pos = 0\n",
    "            key = 0\n",
    "            for d in point:\n",
    "                key += d * math.pow(2, pos)\n",
    "                pos += 1\n",
    "            if key in hash_table:\n",
    "                candidate.append(hash_table[key])\n",
    "    return candidate\n",
    "find_candidate_0_distance(query_binary, hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(hash_code, data_index_set):\n",
    "        self.hash_code = hash_code\n",
    "        self.data_index_set = data_index_set\n",
    "        self.children = []\n",
    "        \n",
    "    def isLeaf():\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def train(dataset):\n",
    "        train_data = dataset[self.data_index_set]\n",
    "        self.model = train(dataset)\n",
    "        \n",
    "    def partition():\n",
    "        points = dataset[self.data_index_set]\n",
    "        hash_table = {}\n",
    "        codes = self.model(points)\n",
    "        for idx, code in enumerate(codes):\n",
    "            if code in hash_table:\n",
    "                hash_table[code].append(self.data_index_set[idx])\n",
    "            else:\n",
    "                hash_table[code] = [self.data_index_set[idx]]\n",
    "        for key,value in d.items():\n",
    "            self.children.append(Node(key, value))\n",
    "    \n",
    "    def search(query, dataset):\n",
    "        if self.isLeaf():\n",
    "            return validate(dataset[self.data_index_set])\n",
    "        else:\n",
    "            children_idxes = select_children(query)\n",
    "            result = []\n",
    "            for idx in children_idxes:\n",
    "                result += self.children[idx].search(query, dataset)\n",
    "            return result\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def index_construction(dataset):\n",
    "    model = train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "data = np.array(f['train'])\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
    "X_tsne = tsne.fit_transform(data[np.random.choice(data.shape[0], 100000, replace=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
