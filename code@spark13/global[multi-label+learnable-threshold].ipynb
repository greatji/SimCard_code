{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data = h5py.File('/home/sunji/ANN/glove_200_angular/glove-200-angular.hdf5', 'r')\n",
    "data_train = np.array(data['train'])\n",
    "data_test = np.array(data['test'])\n",
    "with open('/home/sunji/ANN/glove_200_angular/clusters_glove_200_angular.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)\n",
    "with open('/home/sunji/ANN/glove_200_angular/ground_truth_glove_200_angular_0_4_0_5.pkl', 'rb') as f:\n",
    "    ground_truth_total = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_total_level = [[[] for _ in range(10000)] for _ in range(100)]\n",
    "for clus in range(100):\n",
    "    for t in ground_truth_total[clus]:\n",
    "        ground_truth_total_level[t[0]][t[1]].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for cluster in clusters:\n",
    "    centroids.append(np.mean(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21002\n",
      "7585\n",
      "20072\n",
      "20501\n",
      "9835\n",
      "7053\n",
      "4612\n",
      "8862\n",
      "6927\n",
      "15750\n",
      "12257\n",
      "4633\n",
      "23344\n",
      "7865\n",
      "17241\n",
      "8881\n",
      "7432\n",
      "21209\n",
      "4285\n",
      "4884\n",
      "5573\n",
      "12063\n",
      "8629\n",
      "9347\n",
      "13000\n",
      "18926\n",
      "11938\n",
      "2008\n",
      "9458\n",
      "20117\n",
      "5167\n",
      "24637\n",
      "16506\n",
      "4974\n",
      "9768\n",
      "24978\n",
      "11416\n",
      "10900\n",
      "9872\n",
      "4392\n",
      "8858\n",
      "17615\n",
      "5080\n",
      "12512\n",
      "6673\n",
      "6482\n",
      "10698\n",
      "6538\n",
      "15843\n",
      "10325\n",
      "6219\n",
      "26745\n",
      "28277\n",
      "23462\n",
      "10349\n",
      "9829\n",
      "1290\n",
      "7895\n",
      "5464\n",
      "5025\n",
      "15746\n",
      "10512\n",
      "15796\n",
      "2160\n",
      "1720\n",
      "13290\n",
      "19456\n",
      "7105\n",
      "8093\n",
      "20274\n",
      "7049\n",
      "13444\n",
      "4270\n",
      "14803\n",
      "13106\n",
      "7345\n",
      "18853\n",
      "7240\n",
      "13331\n",
      "10130\n",
      "6414\n",
      "17541\n",
      "5768\n",
      "11370\n",
      "9801\n",
      "1750\n",
      "9711\n",
      "17169\n",
      "12820\n",
      "15282\n",
      "25213\n",
      "11949\n",
      "22146\n",
      "26562\n",
      "5596\n",
      "13140\n",
      "4932\n",
      "12521\n",
      "24137\n",
      "12891\n"
     ]
    }
   ],
   "source": [
    "for c in clusters:\n",
    "    print (len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "\n",
    "def euclidean_dist_normalized(x1, x2=None, eps=1e-8):\n",
    "    if np.isnan(x2):\n",
    "        return 1.0\n",
    "    left = x1\n",
    "    right = x2\n",
    "    return np.sqrt(((left - right) ** 2).mean())\n",
    "\n",
    "def angular_dist(x1, x2=None, eps=1e-8):\n",
    "    cosine_sim = 1 - spatial.distance.cosine(x1, x2)\n",
    "#     print (cosine_sim)\n",
    "    distance = np.arccos(cosine_sim) / 3.14159267\n",
    "    return distance\n",
    "\n",
    "train_features = []\n",
    "train_thresholds = []\n",
    "train_distances = []\n",
    "train_targets = []\n",
    "train_cards = []\n",
    "slot = 0.002\n",
    "for query_id in range(8000):\n",
    "    cardinality = [0 for _ in range(100)]\n",
    "    distances2centroids = []\n",
    "    for cc in centroids:\n",
    "        distances2centroids.append(angular_dist(data_test[query_id], cc))\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.4, 0.5, slot)):\n",
    "        indicator = []\n",
    "        cards = []\n",
    "        for cluster_id in range(100):\n",
    "            cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "            cards.append(cardinality[cluster_id])\n",
    "        feature = data_test[query_id]\n",
    "        train_features.append(feature)\n",
    "        train_distances.append(distances2centroids)\n",
    "        train_thresholds.append([threshold+slot])\n",
    "        train_targets.append(indicator)\n",
    "        train_cards.append(cards)\n",
    "\n",
    "test_features = []\n",
    "test_thresholds = []\n",
    "test_distances = []\n",
    "test_targets = []\n",
    "test_cards = []\n",
    "slot = 0.002\n",
    "for query_id in range(8000,10000):\n",
    "    cardinality = [0 for _ in range(100)]\n",
    "    distances2centroids = []\n",
    "    for cc in centroids:\n",
    "        distances2centroids.append(angular_dist(data_test[query_id], cc))\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.4, 0.5, slot)):\n",
    "        indicator = []\n",
    "        cards = []\n",
    "        for cluster_id in range(100):\n",
    "            cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "            cards.append(cardinality[cluster_id])\n",
    "        feature = data_test[query_id]\n",
    "        test_features.append(feature)\n",
    "        test_distances.append(distances2centroids)\n",
    "        test_thresholds.append([threshold+slot])\n",
    "        test_targets.append(indicator)\n",
    "        test_cards.append(cards)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15839018"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_distances[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(train_features), torch.FloatTensor(train_thresholds), torch.FloatTensor(train_distances), torch.FloatTensor(train_targets), torch.FloatTensor(train_cards)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(test_features), torch.FloatTensor(test_thresholds), torch.FloatTensor(test_distances), torch.FloatTensor(test_targets), torch.FloatTensor(test_cards)), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "input_dimension = 200\n",
    "cluster_dimension = 100\n",
    "hidden_num = 256\n",
    "output_num = 100\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.nn1 = nn.Linear(input_dimension, hidden_num)\n",
    "        self.nn2 = nn.Linear(hidden_num, hidden_num)\n",
    "#         self.nn3 = nn.Linear(hidden_num, hidden_num)\n",
    "        \n",
    "        self.dist1 = nn.Linear(cluster_dimension, hidden_num)\n",
    "        self.dist2 = nn.Linear(hidden_num, hidden_num)\n",
    "        \n",
    "        self.nn4 = nn.Linear(hidden_num, hidden_num)\n",
    "        self.nn5 = nn.Linear(hidden_num, output_num)\n",
    "        \n",
    "        self.thres1 = nn.Linear(1, hidden_num)\n",
    "        self.thres2 = nn.Linear(hidden_num, 1)\n",
    "\n",
    "    def forward(self, x, distances, thresholds):\n",
    "        out1 = F.relu(self.nn1(x))\n",
    "        out2 = F.relu(self.nn2(out1))\n",
    "#         out3 = F.relu(self.nn3(out2))\n",
    "#         print (distances.shape)\n",
    "        distance1 = F.relu(self.dist1(distances))\n",
    "        distance2 = F.relu(self.dist2(distance1))\n",
    "        \n",
    "        thresholds_1 = F.relu(self.thres1(thresholds))\n",
    "        thresholds_2 = self.thres2(thresholds_1)\n",
    "\n",
    "        out4 = F.relu(self.nn4((out2 + distance2) / 2))\n",
    "        out5 = self.nn5(out2)\n",
    "        \n",
    "        probability = F.sigmoid(out5 + thresholds_2)\n",
    "        return probability\n",
    "\n",
    "def loss_fn(estimates, targets, cards):\n",
    "    punish_idx = (estimates < 0.5).float()\n",
    "    return F.mse_loss(estimates, targets) + 0.02 * torch.log(((0.5 - estimates) * cards * punish_idx).mean() + 1.0)\n",
    "\n",
    "def print_loss(estimates, targets, cards):\n",
    "    true_positive = 0.0\n",
    "    true_negative = 0.0\n",
    "    false_positive = 0.0\n",
    "    false_negative = 0.0\n",
    "    num_elements = estimates.shape[1]\n",
    "    for est, tar in zip(estimates, targets):\n",
    "        for i in range(num_elements):\n",
    "            if est[i] < 0.5 and tar[i] == 0:\n",
    "                true_negative += 1\n",
    "            elif est[i] < 0.5 and tar[i] == 1:\n",
    "                false_negative += 1\n",
    "            elif est[i] >= 0.5 and tar[i] == 0:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_positive += 1\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    total_card = cards.sum(dim=1)\n",
    "#     print ('total_card: ', total_card.shape)\n",
    "    miss_card = torch.FloatTensor([cards[i][((estimates[i] < 0.5).nonzero())].sum() for i in range(cards.shape[0])])\n",
    "#     print ('miss_card: ', miss_card.shape)\n",
    "    miss_rate = (miss_card / (total_card + 0.1)).mean()\n",
    "    return precision, recall, miss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunji/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Iteration 0, Batch 0, Loss 0.4127187728881836\n",
      "Training: Iteration 0, Batch 100, Loss 0.04643721505999565\n",
      "Training: Iteration 0, Batch 200, Loss 0.06728208065032959\n",
      "Training: Iteration 0, Batch 300, Loss 0.056348688900470734\n",
      "Training: Iteration 0, Batch 400, Loss 0.05041638761758804\n",
      "Training: Iteration 0, Batch 500, Loss 0.05136996507644653\n",
      "Training: Iteration 0, Batch 600, Loss 0.05564706772565842\n",
      "Training: Iteration 0, Batch 700, Loss 0.05074720457196236\n",
      "Training: Iteration 0, Batch 800, Loss 0.06236615404486656\n",
      "Training: Iteration 0, Batch 900, Loss 0.04720809683203697\n",
      "Training: Iteration 0, Batch 1000, Loss 0.04438183456659317\n",
      "Training: Iteration 0, Batch 1100, Loss 0.0400598868727684\n",
      "Training: Iteration 0, Batch 1200, Loss 0.04799140617251396\n",
      "Training: Iteration 0, Batch 1300, Loss 0.04984555393457413\n",
      "Training: Iteration 0, Batch 1400, Loss 0.04455591365695\n",
      "Training: Iteration 0, Batch 1500, Loss 0.046453915536403656\n",
      "Training: Iteration 0, Batch 1600, Loss 0.0418551005423069\n",
      "Training: Iteration 0, Batch 1700, Loss 0.03354017063975334\n",
      "Training: Iteration 0, Batch 1800, Loss 0.03993476182222366\n",
      "Training: Iteration 0, Batch 1900, Loss 0.02839561551809311\n",
      "Training: Iteration 0, Batch 2000, Loss 0.04019332304596901\n",
      "Training: Iteration 0, Batch 2100, Loss 0.029938314110040665\n",
      "Training: Iteration 0, Batch 2200, Loss 0.033480819314718246\n",
      "Training: Iteration 0, Batch 2300, Loss 0.034365832805633545\n",
      "Training: Iteration 0, Batch 2400, Loss 0.02676667645573616\n",
      "Training: Iteration 0, Batch 2500, Loss 0.025783952325582504\n",
      "Training: Iteration 0, Batch 2600, Loss 0.027883175760507584\n",
      "Training: Iteration 0, Batch 2700, Loss 0.023253168910741806\n",
      "Training: Iteration 0, Batch 2800, Loss 0.032941825687885284\n",
      "Training: Iteration 0, Batch 2900, Loss 0.022517070174217224\n",
      "Training: Iteration 0, Batch 3000, Loss 0.023404350504279137\n",
      "Training: Iteration 0, Batch 3100, Loss 0.02819756232202053\n",
      "Testing: Iteration 0, Batch 0, Loss 0.02930414490401745, Precision 0.9704305578823924, Recall 0.9888187094571527, Miss 0.0007429266697727144\n",
      "Testing: Iteration 0, Batch 100, Loss 0.02751118130981922, Precision 0.973024097376429, Recall 0.9902904494852264, Miss 0.0006227339035831392\n",
      "Testing: Iteration 0, Batch 200, Loss 0.03223366662859917, Precision 0.9625774473358116, Recall 0.9925042589437819, Miss 0.0017475370550528169\n",
      "Testing: Iteration 0, Batch 300, Loss 0.018810130655765533, Precision 0.9831884533377071, Recall 0.9937007874015747, Miss 0.0005168920615687966\n",
      "Testing: Iteration 0, Batch 400, Loss 0.02428870089352131, Precision 0.9770266920089249, Recall 0.9918624161073826, Miss 0.0005346934194676578\n",
      "Testing: Iteration 0, Batch 500, Loss 0.02433810941874981, Precision 0.9771542090886516, Recall 0.9932688262515776, Miss 0.0005544075975194573\n",
      "Testing: Iteration 0, Batch 600, Loss 0.030378609895706177, Precision 0.968413136124342, Recall 0.9901742993848257, Miss 0.0008315282175317407\n",
      "Testing: Iteration 0, Batch 700, Loss 0.02966606244444847, Precision 0.9705316031812474, Recall 0.9890794300827574, Miss 0.0022438750602304935\n",
      "Testing: Loss 0.025644229865535294, Precision 0.9737152519670189, Recall 0.9922557582855864, Miss 0.0006858670385554433\n",
      "Training: Iteration 1, Batch 0, Loss 0.022905347868800163\n",
      "Training: Iteration 1, Batch 100, Loss 0.02462494932115078\n",
      "Training: Iteration 1, Batch 200, Loss 0.024530189111828804\n",
      "Training: Iteration 1, Batch 300, Loss 0.02496400848031044\n",
      "Training: Iteration 1, Batch 400, Loss 0.019648831337690353\n",
      "Training: Iteration 1, Batch 500, Loss 0.028960248455405235\n",
      "Training: Iteration 1, Batch 600, Loss 0.02418231964111328\n",
      "Training: Iteration 1, Batch 700, Loss 0.02885378897190094\n",
      "Training: Iteration 1, Batch 800, Loss 0.022768188267946243\n",
      "Training: Iteration 1, Batch 900, Loss 0.01980043202638626\n",
      "Training: Iteration 1, Batch 1000, Loss 0.01694033481180668\n",
      "Training: Iteration 1, Batch 1100, Loss 0.02108699083328247\n",
      "Training: Iteration 1, Batch 1200, Loss 0.018144480884075165\n",
      "Training: Iteration 1, Batch 1300, Loss 0.020970765501260757\n",
      "Training: Iteration 1, Batch 1400, Loss 0.019541766494512558\n",
      "Training: Iteration 1, Batch 1500, Loss 0.016133958473801613\n",
      "Training: Iteration 1, Batch 1600, Loss 0.01878477819263935\n",
      "Training: Iteration 1, Batch 1700, Loss 0.022610297426581383\n",
      "Training: Iteration 1, Batch 1800, Loss 0.022739319130778313\n",
      "Training: Iteration 1, Batch 1900, Loss 0.02063508704304695\n",
      "Training: Iteration 1, Batch 2000, Loss 0.02092110551893711\n",
      "Training: Iteration 1, Batch 2100, Loss 0.022701671347022057\n",
      "Training: Iteration 1, Batch 2200, Loss 0.017557742074131966\n",
      "Training: Iteration 1, Batch 2300, Loss 0.01684092916548252\n",
      "Training: Iteration 1, Batch 2400, Loss 0.015774568542838097\n",
      "Training: Iteration 1, Batch 2500, Loss 0.01614449918270111\n",
      "Training: Iteration 1, Batch 2600, Loss 0.020287400111556053\n",
      "Training: Iteration 1, Batch 2700, Loss 0.018818583339452744\n",
      "Training: Iteration 1, Batch 2800, Loss 0.018168875947594643\n",
      "Training: Iteration 1, Batch 2900, Loss 0.0187576562166214\n",
      "Training: Iteration 1, Batch 3000, Loss 0.02173306606709957\n",
      "Training: Iteration 1, Batch 3100, Loss 0.020014110952615738\n",
      "Testing: Iteration 1, Batch 0, Loss 0.019099490717053413, Precision 0.9838480207548749, Recall 0.988231338264963, Miss 0.0010296490509063005\n",
      "Testing: Iteration 1, Batch 100, Loss 0.020992262288928032, Precision 0.977497072109754, Recall 0.9905900305188199, Miss 0.0007580131641589105\n",
      "Testing: Iteration 1, Batch 200, Loss 0.013364152982831001, Precision 0.9872305815372102, Recall 0.9934522835161238, Miss 0.00021827442105859518\n",
      "Testing: Iteration 1, Batch 300, Loss 0.026322251185774803, Precision 0.973386279148361, Recall 0.9868094218415417, Miss 0.003927930258214474\n",
      "Testing: Iteration 1, Batch 400, Loss 0.023004813119769096, Precision 0.9775328529037728, Recall 0.9879187730271614, Miss 0.0015220491914078593\n",
      "Testing: Iteration 1, Batch 500, Loss 0.017902767285704613, Precision 0.9825691862866585, Recall 0.9908363878707097, Miss 0.0005864364211447537\n",
      "Testing: Iteration 1, Batch 600, Loss 0.01882316917181015, Precision 0.9829052631578947, Recall 0.9894879620210241, Miss 0.0007132324390113354\n",
      "Testing: Iteration 1, Batch 700, Loss 0.021411646157503128, Precision 0.9808228436259188, Recall 0.987412825310427, Miss 0.0010311886435374618\n",
      "Testing: Loss 0.020209761195913757, Precision 0.9804650418368216, Recall 0.9896142098243107, Miss 0.0015731272287666798\n",
      "Training: Iteration 2, Batch 0, Loss 0.014586696401238441\n",
      "Training: Iteration 2, Batch 100, Loss 0.01968667469918728\n",
      "Training: Iteration 2, Batch 200, Loss 0.015606103464961052\n",
      "Training: Iteration 2, Batch 300, Loss 0.012369224801659584\n",
      "Training: Iteration 2, Batch 400, Loss 0.02035553753376007\n",
      "Training: Iteration 2, Batch 500, Loss 0.01656419411301613\n",
      "Training: Iteration 2, Batch 600, Loss 0.01720018871128559\n",
      "Training: Iteration 2, Batch 700, Loss 0.015874171629548073\n",
      "Training: Iteration 2, Batch 800, Loss 0.011942151933908463\n",
      "Training: Iteration 2, Batch 900, Loss 0.016830965876579285\n",
      "Training: Iteration 2, Batch 1000, Loss 0.01515138614922762\n",
      "Training: Iteration 2, Batch 1100, Loss 0.019705556333065033\n",
      "Training: Iteration 2, Batch 1200, Loss 0.015943076461553574\n",
      "Training: Iteration 2, Batch 1300, Loss 0.016116859391331673\n",
      "Training: Iteration 2, Batch 1400, Loss 0.012301508337259293\n",
      "Training: Iteration 2, Batch 1500, Loss 0.015451465733349323\n",
      "Training: Iteration 2, Batch 1600, Loss 0.018249569460749626\n",
      "Training: Iteration 2, Batch 1700, Loss 0.015757238492369652\n",
      "Training: Iteration 2, Batch 1800, Loss 0.016561312600970268\n",
      "Training: Iteration 2, Batch 1900, Loss 0.01416453905403614\n",
      "Training: Iteration 2, Batch 2000, Loss 0.017848463729023933\n",
      "Training: Iteration 2, Batch 2100, Loss 0.013011164031922817\n",
      "Training: Iteration 2, Batch 2200, Loss 0.01831991970539093\n",
      "Training: Iteration 2, Batch 2300, Loss 0.016743047162890434\n",
      "Training: Iteration 2, Batch 2400, Loss 0.014204771257936954\n",
      "Training: Iteration 2, Batch 2500, Loss 0.0163243617862463\n",
      "Training: Iteration 2, Batch 2600, Loss 0.019628193229436874\n",
      "Training: Iteration 2, Batch 2700, Loss 0.0156809464097023\n",
      "Training: Iteration 2, Batch 2800, Loss 0.013107187114655972\n",
      "Training: Iteration 2, Batch 2900, Loss 0.015310730785131454\n",
      "Training: Iteration 2, Batch 3000, Loss 0.015609899535775185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Iteration 2, Batch 3100, Loss 0.014400960877537727\n",
      "Testing: Iteration 2, Batch 0, Loss 0.020111670717597008, Precision 0.9807370184254607, Recall 0.9889367452073304, Miss 0.0041682105511426926\n",
      "Testing: Iteration 2, Batch 100, Loss 0.01374454703181982, Precision 0.9891918447555883, Recall 0.9901647405950332, Miss 0.0007499120547436178\n",
      "Testing: Iteration 2, Batch 200, Loss 0.016767291352152824, Precision 0.983671449777338, Recall 0.99086226947998, Miss 0.0007325119222514331\n",
      "Testing: Iteration 2, Batch 300, Loss 0.022164126858115196, Precision 0.9777365491651205, Recall 0.9888272921108742, Miss 0.004417243879288435\n",
      "Testing: Iteration 2, Batch 400, Loss 0.021308820694684982, Precision 0.9821596244131455, Recall 0.9856934806819155, Miss 0.002489186590537429\n",
      "Testing: Iteration 2, Batch 500, Loss 0.022497592493891716, Precision 0.9809507611191428, Recall 0.9855604921394395, Miss 0.0016131100710481405\n",
      "Testing: Iteration 2, Batch 600, Loss 0.02194792591035366, Precision 0.9789491411249579, Recall 0.9876815903491633, Miss 0.0021632059942930937\n",
      "Testing: Iteration 2, Batch 700, Loss 0.017091024667024612, Precision 0.9852522205463382, Recall 0.9894807708491121, Miss 0.0018880986608564854\n",
      "Testing: Loss 0.019505670740771704, Precision 0.9818369590873472, Recall 0.988701181592183, Miss 0.002219257177785039\n",
      "Training: Iteration 3, Batch 0, Loss 0.015137222595512867\n",
      "Training: Iteration 3, Batch 100, Loss 0.014855366200208664\n",
      "Training: Iteration 3, Batch 200, Loss 0.014567350037395954\n",
      "Training: Iteration 3, Batch 300, Loss 0.020113402977585793\n",
      "Training: Iteration 3, Batch 400, Loss 0.014925272203981876\n",
      "Training: Iteration 3, Batch 500, Loss 0.015232003293931484\n",
      "Training: Iteration 3, Batch 600, Loss 0.014054933562874794\n",
      "Training: Iteration 3, Batch 700, Loss 0.015775304287672043\n",
      "Training: Iteration 3, Batch 800, Loss 0.012267014943063259\n",
      "Training: Iteration 3, Batch 900, Loss 0.018211832270026207\n",
      "Training: Iteration 3, Batch 1000, Loss 0.022091396152973175\n",
      "Training: Iteration 3, Batch 1100, Loss 0.014567113481462002\n",
      "Training: Iteration 3, Batch 1200, Loss 0.013993161730468273\n",
      "Training: Iteration 3, Batch 1300, Loss 0.0175405815243721\n",
      "Training: Iteration 3, Batch 1400, Loss 0.01304043922573328\n",
      "Training: Iteration 3, Batch 1500, Loss 0.013554507866501808\n",
      "Training: Iteration 3, Batch 1600, Loss 0.015622250735759735\n",
      "Training: Iteration 3, Batch 1700, Loss 0.016737625002861023\n",
      "Training: Iteration 3, Batch 1800, Loss 0.017923664301633835\n",
      "Training: Iteration 3, Batch 1900, Loss 0.01972009241580963\n",
      "Training: Iteration 3, Batch 2000, Loss 0.0195982214063406\n",
      "Training: Iteration 3, Batch 2100, Loss 0.015257484279572964\n",
      "Training: Iteration 3, Batch 2200, Loss 0.0177460964769125\n",
      "Training: Iteration 3, Batch 2300, Loss 0.018550606444478035\n",
      "Training: Iteration 3, Batch 2400, Loss 0.012107908725738525\n",
      "Training: Iteration 3, Batch 2500, Loss 0.01881568878889084\n",
      "Training: Iteration 3, Batch 2600, Loss 0.014164763502776623\n",
      "Training: Iteration 3, Batch 2700, Loss 0.014899274334311485\n",
      "Training: Iteration 3, Batch 2800, Loss 0.017319142818450928\n",
      "Training: Iteration 3, Batch 2900, Loss 0.015072392299771309\n",
      "Training: Iteration 3, Batch 3000, Loss 0.01151387207210064\n",
      "Training: Iteration 3, Batch 3100, Loss 0.014140784740447998\n",
      "Testing: Iteration 3, Batch 0, Loss 0.021500660106539726, Precision 0.9836942675159236, Recall 0.9848652325482526, Miss 0.0040909359231591225\n",
      "Testing: Iteration 3, Batch 100, Loss 0.01705949194729328, Precision 0.9846461949265688, Recall 0.99026518966096, Miss 0.0014892873587086797\n",
      "Testing: Iteration 3, Batch 200, Loss 0.022854631766676903, Precision 0.9807611172052991, Recall 0.985, Miss 0.003637485671788454\n",
      "Testing: Iteration 3, Batch 300, Loss 0.019072510302066803, Precision 0.9846625766871165, Recall 0.9882675986020969, Miss 0.0019374288385733962\n",
      "Testing: Iteration 3, Batch 400, Loss 0.017801407724618912, Precision 0.9845704753961635, Recall 0.9888591053777852, Miss 0.0014797174371778965\n",
      "Testing: Iteration 3, Batch 500, Loss 0.021624600514769554, Precision 0.9805244077227889, Recall 0.9874341993547292, Miss 0.0016232128255069256\n",
      "Testing: Iteration 3, Batch 600, Loss 0.02346332184970379, Precision 0.9793911407559432, Recall 0.984526777271555, Miss 0.005061717238277197\n",
      "Testing: Iteration 3, Batch 700, Loss 0.016382714733481407, Precision 0.98737960810362, Recall 0.9887752556747319, Miss 0.0018288802821189165\n",
      "Testing: Loss 0.019758747574989983, Precision 0.9833955573719209, Recall 0.986988461540605, Miss 0.0024779518134891987\n",
      "Training: Iteration 4, Batch 0, Loss 0.011119828559458256\n",
      "Training: Iteration 4, Batch 100, Loss 0.016740383580327034\n",
      "Training: Iteration 4, Batch 200, Loss 0.016797196120023727\n",
      "Training: Iteration 4, Batch 300, Loss 0.017960142344236374\n",
      "Training: Iteration 4, Batch 400, Loss 0.01577286422252655\n",
      "Training: Iteration 4, Batch 500, Loss 0.011924011632800102\n",
      "Training: Iteration 4, Batch 600, Loss 0.01327483355998993\n",
      "Training: Iteration 4, Batch 700, Loss 0.0157499760389328\n",
      "Training: Iteration 4, Batch 800, Loss 0.014665388502180576\n",
      "Training: Iteration 4, Batch 900, Loss 0.011990699917078018\n",
      "Training: Iteration 4, Batch 1000, Loss 0.01264597661793232\n",
      "Training: Iteration 4, Batch 1100, Loss 0.01501015480607748\n",
      "Training: Iteration 4, Batch 1200, Loss 0.010471965186297894\n",
      "Training: Iteration 4, Batch 1300, Loss 0.012255264446139336\n",
      "Training: Iteration 4, Batch 1400, Loss 0.01668498106300831\n",
      "Training: Iteration 4, Batch 1500, Loss 0.01639186218380928\n",
      "Training: Iteration 4, Batch 1600, Loss 0.01481995265930891\n",
      "Training: Iteration 4, Batch 1700, Loss 0.01604735665023327\n",
      "Training: Iteration 4, Batch 1800, Loss 0.018080106005072594\n",
      "Training: Iteration 4, Batch 1900, Loss 0.013583486899733543\n",
      "Training: Iteration 4, Batch 2000, Loss 0.014057090505957603\n",
      "Training: Iteration 4, Batch 2100, Loss 0.016383975744247437\n",
      "Training: Iteration 4, Batch 2200, Loss 0.017183776944875717\n",
      "Training: Iteration 4, Batch 2300, Loss 0.018217435106635094\n",
      "Training: Iteration 4, Batch 2400, Loss 0.014353934675455093\n",
      "Training: Iteration 4, Batch 2500, Loss 0.016438568010926247\n",
      "Training: Iteration 4, Batch 2600, Loss 0.015191401354968548\n",
      "Training: Iteration 4, Batch 2700, Loss 0.012814956717193127\n",
      "Training: Iteration 4, Batch 2800, Loss 0.014890702441334724\n",
      "Training: Iteration 4, Batch 2900, Loss 0.012460741214454174\n",
      "Training: Iteration 4, Batch 3000, Loss 0.018394915387034416\n",
      "Training: Iteration 4, Batch 3100, Loss 0.017089292407035828\n",
      "Testing: Iteration 4, Batch 0, Loss 0.01793649047613144, Precision 0.9872877812160241, Recall 0.9862979363355334, Miss 0.0018006454920396209\n",
      "Testing: Iteration 4, Batch 100, Loss 0.021327754482626915, Precision 0.9821458368096112, Recall 0.9870042760124088, Miss 0.0016493446892127395\n",
      "Testing: Iteration 4, Batch 200, Loss 0.018437830731272697, Precision 0.9867549668874173, Recall 0.9851033559293665, Miss 0.0019050120608881116\n",
      "Testing: Iteration 4, Batch 300, Loss 0.02260039560496807, Precision 0.9824236817761333, Recall 0.984410550265442, Miss 0.002466973615810275\n",
      "Testing: Iteration 4, Batch 400, Loss 0.017071079462766647, Precision 0.9854770052583257, Recall 0.9903539674551249, Miss 0.0011320386547595263\n",
      "Testing: Iteration 4, Batch 500, Loss 0.01827351190149784, Precision 0.9859273615909665, Recall 0.9872584592017551, Miss 0.0012422705767676234\n",
      "Testing: Iteration 4, Batch 600, Loss 0.02016706019639969, Precision 0.9851782840687727, Recall 0.9850114319586756, Miss 0.009363323450088501\n",
      "Testing: Iteration 4, Batch 700, Loss 0.023050235584378242, Precision 0.9791029561671764, Recall 0.9869840726151738, Miss 0.0026354906149208546\n",
      "Testing: Loss 0.02015728920536197, Precision 0.9838830852461945, Recall 0.9861769419630667, Miss 0.0028223171830177307\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "for e in range(5):\n",
    "    model.train()\n",
    "    for batch_idx, (features, thresholds, distances, targets, cards) in enumerate(train_loader):\n",
    "        x = Variable(features)\n",
    "        y = Variable(targets.unsqueeze(1))\n",
    "        z = Variable(thresholds)\n",
    "        dists = Variable(distances)\n",
    "        opt.zero_grad()\n",
    "        estimates = model(x, dists, z)\n",
    "        loss = loss_fn(estimates, targets, cards)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Training: Iteration {0}, Batch {1}, Loss {2}'.format(e, batch_idx, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        next(model.thres1.parameters()).data.clamp_(0)\n",
    "        next(model.thres2.parameters()).data.clamp_(0)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    miss_rate = 0.0\n",
    "    for batch_idx, (features, thresholds, distances, targets, cards) in enumerate(test_loader):\n",
    "        x = Variable(features)\n",
    "        y = Variable(targets.unsqueeze(1))\n",
    "        z = Variable(thresholds)\n",
    "        dists = Variable(distances)\n",
    "        estimates = model(x, dists, z)\n",
    "        loss = loss_fn(estimates, targets, cards)\n",
    "        test_loss += loss.item()\n",
    "        prec, rec, miss = print_loss(estimates, targets, cards)\n",
    "        precision += prec\n",
    "        recall += rec\n",
    "        miss_rate += miss\n",
    "        if batch_idx % 100 == 0:\n",
    "            print ('Testing: Iteration {0}, Batch {1}, Loss {2}, Precision {3}, Recall {4}, Miss {5}'.format(e, batch_idx, loss.item(), prec, rec, miss))\n",
    "    test_loss /= len(test_loader)\n",
    "    precision /= len(test_loader)\n",
    "    recall /= len(test_loader)\n",
    "    miss_rate /= len(test_loader)\n",
    "    print ('Testing: Loss {0}, Precision {1}, Recall {2}, Miss {3}'.format(test_loss, precision, recall, miss_rate))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(distances[2], features[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.thres1.named_parameters():\n",
    "    print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/sunji/ANN/glove_200_angular/global_glove_200_angular_punish_query_threshold_monotonic.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-84400519e6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011616033501923084 0.9865168539325843 0.9653655854865311 tensor(0.0307)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013758893124759197 0.9868929952728835 0.9659305993690852 tensor(0.0149)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.010944121517241001 0.9889834752128193 0.9657701711491442 tensor(0.0170)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.012130827642977238 0.9878915504080021 0.9596011250319612 tensor(0.0179)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011950699612498283 0.9855285749325484 0.972645848462842 tensor(0.0386)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014175038784742355 0.9893148962916405 0.9622987568779295 tensor(0.0226)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014790528453886509 0.9867578900904878 0.9652417962003454 tensor(0.0361)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01132373046129942 0.9918009612666101 0.9592562209461307 tensor(0.0334)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01490761712193489 0.9856217091940057 0.9689428628309775 tensor(0.0043)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013611814007163048 0.9878243512974052 0.9664128099980472 tensor(0.0165)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01533545833081007 0.9882304356803634 0.9620100502512563 tensor(0.0020)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013402558863162994 0.985386690647482 0.9658439841339798 tensor(0.0369)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.012438501231372356 0.9876464323748669 0.9715063901110412 tensor(0.0238)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011510196141898632 0.9892645043398812 0.9676050044682752 tensor(0.0215)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011357973329722881 0.9818237831176833 0.963422007255139 tensor(0.0103)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011999377980828285 0.9858916478555305 0.9622693472872487 tensor(0.0185)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011021489277482033 0.9905447996398019 0.9678838539375275 tensor(0.0184)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01258517149835825 0.985634477254589 0.9671104150352389 tensor(0.0303)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.0109231136739254 0.9849644614543467 0.970113085621971 tensor(0.0210)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014314117841422558 0.9891278086494323 0.9549801726148822 tensor(0.0079)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.010538735426962376 0.985855013892397 0.9689672293942403 tensor(0.0122)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011180868372321129 0.988537880905787 0.9637503406922867 tensor(0.0323)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a20c1e944ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print (targets[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-dff79b58f867>\u001b[0m in \u001b[0;36mprint_loss\u001b[0;34m(estimates, targets, cards)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mfalse_positive\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mfalse_negative\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, thresholds, distances, targets, cards) in enumerate(test_loader):\n",
    "    x = Variable(features)\n",
    "    y = Variable(targets.unsqueeze(1))\n",
    "    z = Variable(thresholds)\n",
    "    dist = Variable(distances)\n",
    "    estimates = model(x, dist, z)\n",
    "    loss = loss_fn(estimates, targets, cards)\n",
    "    prec, rec, miss_rate = print_loss(estimates, targets, cards)\n",
    "    print (loss.item(), prec, rec, miss_rate)\n",
    "#     print (targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#     torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_dataset = np.array(f['train'])\n",
    "test_dataset = np.array(f['test'])\n",
    "train_lefts, train_rights, test_lefts, test_rights = prepare_dataset(train_dataset, test_dataset, train_num, test_num)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    (train_lefts, train_rights), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    (test_lefts, test_rights), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_distances, input_distances = test(model, device, train_loader)\n",
    "hash_distances, input_distances = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = torch.FloatTensor([f['train'][0] for x in range(999)])\n",
    "rights = torch.FloatTensor(f['train'][1:1000])\n",
    "inputdistance = angular_distance(lefts, rights).detach().numpy()\n",
    "hashdistance = l1_distance(model(lefts), model(rights)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xx in zip(inputdistance, hashdistance):\n",
    "#     print (xx[0], xx[1])\n",
    "index_1 = np.argsort(hashdistance, 0)\n",
    "index_2 = np.argsort(inputdistance, 0)\n",
    "# np.random.shuffle(index_2)\n",
    "\n",
    "input_index = {}\n",
    "for pos, idx in enumerate(index_2):\n",
    "    input_index[idx] = pos\n",
    "sum = 0.0\n",
    "for pos, idx in enumerate(index_1):\n",
    "    sum += np.abs(pos - input_index[idx])\n",
    "sum / len(index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.sort(inputdistance, 0)\n",
    "plt.plot(xxx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "distances = []\n",
    "for i in index_1:\n",
    "    distances.append(math.floor(inputdistance[i].item()* 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(torch.FloatTensor(f['train'][0]).unsqueeze(0), torch.FloatTensor(f['train'][6]).unsqueeze(0), dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(hash_distances[0][0:30], input_distances[0][0:30]):\n",
    "    print (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vector = model(torch.FloatTensor(f['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = model(torch.FloatTensor(f['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(vector):\n",
    "    query_codes = []\n",
    "    for v in vector:\n",
    "        binary_code = []\n",
    "        for e in v:\n",
    "            if e < 0.5:\n",
    "                binary_code.append(0)\n",
    "            else:\n",
    "                binary_code.append(1)\n",
    "        query_codes.append(binary_code)\n",
    "    return np.array(query_codes)\n",
    "dataset_binary = binarization(dataset_vector.detach().numpy())\n",
    "query_binary = binarization(query_vector.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "hash_table = {}\n",
    "for idx, point in enumerate(dataset_binary):\n",
    "    pos = 0\n",
    "    key = 0\n",
    "    for d in point:\n",
    "        key += d * math.pow(2, pos)\n",
    "        pos += 1\n",
    "    if key in hash_table:\n",
    "        hash_table[key].append(idx)\n",
    "    else:\n",
    "        hash_table[key] = [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['neighbors'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidate_distance(vector, hash_table, candidate_num):\n",
    "    candidate = []\n",
    "    for point in query_binary:\n",
    "        cand = []\n",
    "        dis = 0\n",
    "        while len(cand) < 100:\n",
    "            pos = 0\n",
    "            key = 0\n",
    "            for d in point:\n",
    "                key += d * math.pow(2, pos)\n",
    "                pos += 1\n",
    "            if key in hash_table:\n",
    "                candidate.append(hash_table[key])\n",
    "    return candidate\n",
    "find_candidate_0_distance(query_binary, hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(hash_code, data_index_set):\n",
    "        self.hash_code = hash_code\n",
    "        self.data_index_set = data_index_set\n",
    "        self.children = []\n",
    "        \n",
    "    def isLeaf():\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def train(dataset):\n",
    "        train_data = dataset[self.data_index_set]\n",
    "        self.model = train(dataset)\n",
    "        \n",
    "    def partition():\n",
    "        points = dataset[self.data_index_set]\n",
    "        hash_table = {}\n",
    "        codes = self.model(points)\n",
    "        for idx, code in enumerate(codes):\n",
    "            if code in hash_table:\n",
    "                hash_table[code].append(self.data_index_set[idx])\n",
    "            else:\n",
    "                hash_table[code] = [self.data_index_set[idx]]\n",
    "        for key,value in d.items():\n",
    "            self.children.append(Node(key, value))\n",
    "    \n",
    "    def search(query, dataset):\n",
    "        if self.isLeaf():\n",
    "            return validate(dataset[self.data_index_set])\n",
    "        else:\n",
    "            children_idxes = select_children(query)\n",
    "            result = []\n",
    "            for idx in children_idxes:\n",
    "                result += self.children[idx].search(query, dataset)\n",
    "            return result\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def index_construction(dataset):\n",
    "    model = train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "data = np.array(f['train'])\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
    "X_tsne = tsne.fit_transform(data[np.random.choice(data.shape[0], 100000, replace=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
