{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/glove_200_angular/glove-200-angular.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/glove_200_angular/clusters_glove_200_angular.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/glove_200_angular/ground_truth_glove_200_angular_0_4_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "    \n",
    "# data = h5py.File('/home/sunji/ANN/fashion_mnist_784_euclidean/fashion-mnist-784-euclidean.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/fashion_mnist_784_euclidean/clusters_fashion_mnist_784_euclidean.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/fashion_mnist_784_euclidean/ground_truth_fashion_mnist_784_euclidean_0_0_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/nytimes_256_angular/nytimes-256-angular.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/nytimes_256_angular/clusters_nytimes_256_angular.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/nytimes_256_angular/ground_truth_nytimes_256_angular_0_4_0_5.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "\n",
    "# data = h5py.File('/home/sunji/ANN/sift_128_euclidean/sift-128-euclidean.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/sift_128_euclidean/clusters_sift_128_euclidean.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/sift_128_euclidean/ground_truth_sift_128_euclidean_0_0_0_2.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "    \n",
    "# data = h5py.File('/home/sunji/ANN/kosarak_jaccard/kosarak-jaccard.hdf5', 'r')\n",
    "# data_train = np.array(data['train'])\n",
    "# data_test = np.array(data['test'])\n",
    "# with open('/home/sunji/ANN/kosarak_jaccard/clusters_kosarak_jaccard.pkl', 'rb') as f:\n",
    "#     clusters = pickle.load(f)\n",
    "# with open('/home/sunji/ANN/kosarak_jaccard/ground_truth_kosarak_jaccard_0_9_1_0.pkl', 'rb') as f:\n",
    "#     ground_truth_total = pickle.load(f)\n",
    "\n",
    "data = h5py.File('/home/sunji/ANN/gist_960_euclidean/gist-960-euclidean.hdf5', 'r')\n",
    "data_train = np.array(data['train'])\n",
    "data_test = np.array(data['test'])\n",
    "with open('/home/sunji/ANN/gist_960_euclidean/clusters_gist_960_euclidean.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)\n",
    "with open('/home/sunji/ANN/gist_960_euclidean/ground_truth_gist_960_euclidean_0_0_0_1.pkl', 'rb') as f:\n",
    "    ground_truth_total = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_total_level = [[[] for _ in range(1000)] for _ in range(cluster_size)]\n",
    "for clus in range(cluster_size):\n",
    "    for t in ground_truth_total[clus]:\n",
    "        ground_truth_total_level[t[0]][t[1]].append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for cluster in clusters:\n",
    "    centroids.append(np.mean(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "\n",
    "def euclidean_dist_normalized(x1, x2=None, eps=1e-8):\n",
    "    if np.isnan(x2):\n",
    "        return 1.0\n",
    "    left = x1\n",
    "    right = x2\n",
    "    return np.sqrt(((left - right) ** 2).mean())\n",
    "\n",
    "def angular_dist(x1, x2=None, eps=1e-8):\n",
    "    cosine_sim = 1 - spatial.distance.cosine(x1, x2)\n",
    "#     print (cosine_sim)\n",
    "    distance = np.arccos(cosine_sim) / 3.14159267\n",
    "    return distance\n",
    "\n",
    "train_features = []\n",
    "train_thresholds = []\n",
    "train_targets = []\n",
    "train_cards = []\n",
    "slot = 0.001\n",
    "for train_id in range(800):\n",
    "    cardinality = [0 for _ in range(cluster_size)]\n",
    "    query_ids = np.random.choice(800, 30, replace=False)\n",
    "#     queries = []\n",
    "#     for query_id in query_ids:\n",
    "#         queries.append(data_test[query_id])\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.0, 0.1, slot)):\n",
    "        indicator = []\n",
    "        cards = []\n",
    "        for cluster_id in range(cluster_size):\n",
    "            for query_id in query_ids:\n",
    "                cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "            cards.append(cardinality[cluster_id])\n",
    "        train_features.append(query_ids)\n",
    "        train_thresholds.append([threshold+slot])\n",
    "        train_targets.append(indicator)\n",
    "        train_cards.append(cards)\n",
    "\n",
    "test_features = []\n",
    "test_thresholds = []\n",
    "test_targets = []\n",
    "test_cards = []\n",
    "for test_id in range(200):\n",
    "    cardinality = [0 for _ in range(cluster_size)]\n",
    "    query_ids = np.random.choice(200, 30, replace=False) + 800\n",
    "#     queries = []\n",
    "#     for query_id in query_ids:\n",
    "#         queries.append(data_test[query_id])\n",
    "    for threshold_id, threshold in enumerate(np.arange(0.0, 0.1, slot)):\n",
    "        indicator = []\n",
    "        cards = []\n",
    "        for cluster_id in range(cluster_size):\n",
    "            for query_id in query_ids:\n",
    "                cardinality[cluster_id] += ground_truth_total_level[cluster_id][query_id][threshold_id][-1]\n",
    "            if cardinality[cluster_id] > 0:\n",
    "                indicator.append(1)\n",
    "            else:\n",
    "                indicator.append(0)\n",
    "            cards.append(cardinality[cluster_id])\n",
    "        test_features.append(query_ids)\n",
    "        test_thresholds.append([threshold+slot])\n",
    "        test_targets.append(indicator)\n",
    "        test_cards.append(cards)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8767  616 5086 3584 7169 9913 1754  753 2437 8300 6602 2495 7491 6415\n",
      " 7751 6538 2632 9530 6118 5148 2524 3453 1302  420 2507  276 2582 1841\n",
      " 5304 8075]\n",
      "[2904 9979 1583 7084 7576 1973 7912 3251 1198  117 1745 3507 7229 1735\n",
      " 3633 1291 4440 2013 2226 4076 6892 9597  258 1422 1700 2703 1742 2114\n",
      " 5352 8895]\n",
      "[5387 6360 9812 6131  449 9745 8533  988 7473  210 3270 4829 6187 4762\n",
      " 9859 6138 7068 4879 5949 1850 7460 8457 3986  414 8320 3283 4815 7148\n",
      " 5698 2288]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print (np.random.choice(10000, 30, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(train_features), torch.FloatTensor(train_thresholds), torch.FloatTensor(train_targets), torch.FloatTensor(train_cards)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.FloatTensor(test_features), torch.FloatTensor(test_thresholds), torch.FloatTensor(test_targets), torch.FloatTensor(test_cards)), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "input_dimension = 960\n",
    "cluster_dimension = cluster_size\n",
    "hidden_num = 256\n",
    "output_num = cluster_size\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.emb_nn1 = nn.Linear(input_dimension, hidden_num)\n",
    "        self.emb_nn2 = nn.Linear(hidden_num, input_dimension)\n",
    "        \n",
    "        self.nn1 = nn.Linear(input_dimension, hidden_num)\n",
    "        self.nn2 = nn.Linear(hidden_num, hidden_num)\n",
    "#         self.nn3 = nn.Linear(hidden_num, hidden_num)\n",
    "        \n",
    "#         self.dist1 = nn.Linear(cluster_dimension, hidden_num)\n",
    "#         self.dist2 = nn.Linear(hidden_num, hidden_num)\n",
    "        \n",
    "#         self.nn4 = nn.Linear(hidden_num, hidden_num)\n",
    "        self.nn5 = nn.Linear(hidden_num, output_num)\n",
    "        \n",
    "        self.thres1 = nn.Linear(1, hidden_num)\n",
    "        self.thres2 = nn.Linear(hidden_num, 1)\n",
    "\n",
    "    def forward(self, queries, thresholds):\n",
    "        batch_size = queries.shape[0]\n",
    "        set_size = queries.shape[1]\n",
    "        queries = queries.view(batch_size * set_size, -1)\n",
    "        \n",
    "        queries = F.relu(self.emb_nn1(queries))\n",
    "        queries = F.relu(self.emb_nn2(queries))\n",
    "        queries = queries.view(batch_size, set_size, -1).mean(dim=1)\n",
    "        \n",
    "        out1 = F.relu(self.nn1(queries))\n",
    "        out2 = F.relu(self.nn2(out1))\n",
    "#         out3 = F.relu(self.nn3(out2))\n",
    "#         print (distances.shape)\n",
    "#         distance1 = F.relu(self.dist1(distances))\n",
    "#         distance2 = F.relu(self.dist2(distance1))\n",
    "        \n",
    "        thresholds_1 = F.relu(self.thres1(thresholds))\n",
    "        thresholds_2 = self.thres2(thresholds_1)\n",
    "\n",
    "#         out4 = F.relu(self.nn4((out2 + distance2) / 2))\n",
    "        out5 = self.nn5(out2)\n",
    "        \n",
    "        probability = F.sigmoid(out5 + thresholds_2)\n",
    "        return probability\n",
    "\n",
    "def loss_fn(estimates, targets, cards):\n",
    "    punish_idx = (estimates < 0.5).float()\n",
    "    return F.mse_loss(estimates, targets) + 0.02 * torch.log(((0.5 - estimates) * cards * punish_idx).mean() + 1.0)\n",
    "\n",
    "def print_loss(estimates, targets, cards):\n",
    "    true_positive = 0.0\n",
    "    true_negative = 0.0\n",
    "    false_positive = 0.0\n",
    "    false_negative = 0.0\n",
    "    num_elements = estimates.shape[1]\n",
    "    for est, tar in zip(estimates, targets):\n",
    "        for i in range(num_elements):\n",
    "            if est[i] < 0.5 and tar[i] == 0:\n",
    "                true_negative += 1\n",
    "            elif est[i] < 0.5 and tar[i] == 1:\n",
    "                false_negative += 1\n",
    "            elif est[i] >= 0.5 and tar[i] == 0:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_positive += 1\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    total_card = cards.sum(dim=1)\n",
    "#     print ('total_card: ', total_card.shape)\n",
    "    miss_card = torch.FloatTensor([cards[i][((estimates[i] < 0.5).nonzero())].sum() for i in range(cards.shape[0])])\n",
    "#     print ('miss_card: ', miss_card.shape)\n",
    "    miss_rate = (miss_card / (total_card + 0.1)).mean()\n",
    "    return precision, recall, miss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunji/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Iteration 0, Batch 0, Loss 0.40437230467796326\n",
      "Training: Iteration 0, Batch 100, Loss 0.22659610211849213\n",
      "Training: Iteration 0, Batch 200, Loss 0.18407416343688965\n",
      "Training: Iteration 0, Batch 300, Loss 0.20684599876403809\n",
      "Training: Iteration 0, Batch 400, Loss 0.20094846189022064\n",
      "Training: Iteration 0, Batch 500, Loss 0.1804567128419876\n",
      "Training: Iteration 0, Batch 600, Loss 0.15404315292835236\n",
      "Testing: Iteration 0, Batch 0, Loss 0.1489168107509613, Precision 0.7724590163934426, Recall 0.9961343319642426, Miss 0.015953045338392258\n",
      "Testing: Iteration 0, Batch 100, Loss 0.146820530295372, Precision 0.7854949421965318, Recall 0.9958777052559258, Miss 5.017125204176409e-06\n",
      "Testing: Loss 0.1545295648893733, Precision 0.7533175598102643, Recall 0.9949581612236158, Miss 0.012418574653565884\n",
      "Training: Iteration 1, Batch 0, Loss 0.1505347341299057\n",
      "Training: Iteration 1, Batch 100, Loss 0.13088864088058472\n",
      "Training: Iteration 1, Batch 200, Loss 0.11520937830209732\n",
      "Training: Iteration 1, Batch 300, Loss 0.10703974962234497\n",
      "Training: Iteration 1, Batch 400, Loss 0.08851451426744461\n",
      "Training: Iteration 1, Batch 500, Loss 0.08061573654413223\n",
      "Training: Iteration 1, Batch 600, Loss 0.06673932075500488\n",
      "Testing: Iteration 1, Batch 0, Loss 0.06701518595218658, Precision 0.9540571428571428, Recall 0.9893339653946432, Miss 0.09443853795528412\n",
      "Testing: Iteration 1, Batch 100, Loss 0.07419905066490173, Precision 0.9326511185149928, Recall 0.9891468955073195, Miss 0.09599395096302032\n",
      "Testing: Loss 0.07195582898084525, Precision 0.94462087747237, Recall 0.9885666711007628, Miss 0.07967191189527512\n",
      "Training: Iteration 2, Batch 0, Loss 0.08548388630151749\n",
      "Training: Iteration 2, Batch 100, Loss 0.06269213557243347\n",
      "Training: Iteration 2, Batch 200, Loss 0.06093393266201019\n",
      "Training: Iteration 2, Batch 300, Loss 0.05479830503463745\n",
      "Training: Iteration 2, Batch 400, Loss 0.05029771476984024\n",
      "Training: Iteration 2, Batch 500, Loss 0.04802054539322853\n",
      "Training: Iteration 2, Batch 600, Loss 0.04646400362253189\n",
      "Testing: Iteration 2, Batch 0, Loss 0.05224477872252464, Precision 0.9803616333373248, Recall 0.9695641875888205, Miss 0.08557065576314926\n",
      "Testing: Iteration 2, Batch 100, Loss 0.043880097568035126, Precision 0.9754380925021545, Recall 0.9694503925767309, Miss 0.12342724949121475\n",
      "Testing: Loss 0.04766587787278139, Precision 0.9806853329688885, Recall 0.971479228387344, Miss 0.08673761039972305\n",
      "Training: Iteration 3, Batch 0, Loss 0.039618730545043945\n",
      "Training: Iteration 3, Batch 100, Loss 0.041665274649858475\n",
      "Training: Iteration 3, Batch 200, Loss 0.042254507541656494\n",
      "Training: Iteration 3, Batch 300, Loss 0.04620939493179321\n",
      "Training: Iteration 3, Batch 400, Loss 0.03742845356464386\n",
      "Training: Iteration 3, Batch 500, Loss 0.03909279778599739\n",
      "Training: Iteration 3, Batch 600, Loss 0.04015479236841202\n",
      "Testing: Iteration 3, Batch 0, Loss 0.03317541256546974, Precision 0.9711396151948692, Recall 0.9873354231974921, Miss 0.13794472813606262\n",
      "Testing: Iteration 3, Batch 100, Loss 0.038105882704257965, Precision 0.9640395940358351, Recall 0.9842650633235257, Miss 0.13361915946006775\n",
      "Testing: Loss 0.037550681525734576, Precision 0.9652672762649557, Recall 0.9855881877096337, Miss 0.08844581246376038\n",
      "Training: Iteration 4, Batch 0, Loss 0.03148148953914642\n",
      "Training: Iteration 4, Batch 100, Loss 0.034365978091955185\n",
      "Training: Iteration 4, Batch 200, Loss 0.02772442437708378\n",
      "Training: Iteration 4, Batch 300, Loss 0.032779090106487274\n",
      "Training: Iteration 4, Batch 400, Loss 0.03837317228317261\n",
      "Training: Iteration 4, Batch 500, Loss 0.03259888291358948\n",
      "Training: Iteration 4, Batch 600, Loss 0.029776887968182564\n",
      "Testing: Iteration 4, Batch 0, Loss 0.031276948750019073, Precision 0.9719917012448133, Recall 0.9837844143723752, Miss 0.0652778223156929\n",
      "Testing: Iteration 4, Batch 100, Loss 0.03491773456335068, Precision 0.9672514619883041, Recall 0.9763872491145218, Miss 0.09830509126186371\n",
      "Testing: Loss 0.032532330270215966, Precision 0.9696456860401577, Recall 0.9822286121310139, Miss 0.08474063128232956\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "for e in range(5):\n",
    "    model.train()\n",
    "    for batch_idx, (features, thresholds, targets, cards) in enumerate(train_loader):\n",
    "#         print (features)\n",
    "        queries = torch.FloatTensor(data_test[features.type(torch.LongTensor)].astype(float))\n",
    "        x = Variable(queries)\n",
    "        y = Variable(targets.unsqueeze(1))\n",
    "        z = Variable(thresholds)\n",
    "        opt.zero_grad()\n",
    "        estimates = model(x, z)\n",
    "        loss = loss_fn(estimates, targets, cards)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Training: Iteration {0}, Batch {1}, Loss {2}'.format(e, batch_idx, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        next(model.thres1.parameters()).data.clamp_(0)\n",
    "        next(model.thres2.parameters()).data.clamp_(0)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    miss_rate = 0.0\n",
    "    for batch_idx, (features, thresholds, targets, cards) in enumerate(test_loader):\n",
    "        queries = torch.FloatTensor(data_test[features.type(torch.LongTensor)].astype(float))\n",
    "        x = Variable(queries)\n",
    "        y = Variable(targets.unsqueeze(1))\n",
    "        z = Variable(thresholds)\n",
    "        estimates = model(x, z)\n",
    "        loss = loss_fn(estimates, targets, cards)\n",
    "        test_loss += loss.item()\n",
    "        prec, rec, miss = print_loss(estimates, targets, cards)\n",
    "        precision += prec\n",
    "        recall += rec\n",
    "        miss_rate += miss\n",
    "        if batch_idx % 100 == 0:\n",
    "            print ('Testing: Iteration {0}, Batch {1}, Loss {2}, Precision {3}, Recall {4}, Miss {5}'.format(e, batch_idx, loss.item(), prec, rec, miss))\n",
    "    test_loss /= len(test_loader)\n",
    "    precision /= len(test_loader)\n",
    "    recall /= len(test_loader)\n",
    "    miss_rate /= len(test_loader)\n",
    "    print ('Testing: Loss {0}, Precision {1}, Recall {2}, Miss {3}'.format(test_loss, precision, recall, miss_rate))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(distances[2], features[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.thres1.named_parameters():\n",
    "    print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/sunji/ANN/join/gist_960_euclidean/global_gist_960_euclidean_punish_query_threshold_monotonic.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-84400519e6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('/home/sunji/ANN/sift_128_euclidean/global_sift_128_euclidean_binary_query_threshold.model'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011616033501923084 0.9865168539325843 0.9653655854865311 tensor(0.0307)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013758893124759197 0.9868929952728835 0.9659305993690852 tensor(0.0149)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.010944121517241001 0.9889834752128193 0.9657701711491442 tensor(0.0170)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.012130827642977238 0.9878915504080021 0.9596011250319612 tensor(0.0179)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011950699612498283 0.9855285749325484 0.972645848462842 tensor(0.0386)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014175038784742355 0.9893148962916405 0.9622987568779295 tensor(0.0226)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014790528453886509 0.9867578900904878 0.9652417962003454 tensor(0.0361)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01132373046129942 0.9918009612666101 0.9592562209461307 tensor(0.0334)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01490761712193489 0.9856217091940057 0.9689428628309775 tensor(0.0043)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013611814007163048 0.9878243512974052 0.9664128099980472 tensor(0.0165)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01533545833081007 0.9882304356803634 0.9620100502512563 tensor(0.0020)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.013402558863162994 0.985386690647482 0.9658439841339798 tensor(0.0369)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.012438501231372356 0.9876464323748669 0.9715063901110412 tensor(0.0238)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011510196141898632 0.9892645043398812 0.9676050044682752 tensor(0.0215)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011357973329722881 0.9818237831176833 0.963422007255139 tensor(0.0103)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011999377980828285 0.9858916478555305 0.9622693472872487 tensor(0.0185)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011021489277482033 0.9905447996398019 0.9678838539375275 tensor(0.0184)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.01258517149835825 0.985634477254589 0.9671104150352389 tensor(0.0303)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.0109231136739254 0.9849644614543467 0.970113085621971 tensor(0.0210)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.014314117841422558 0.9891278086494323 0.9549801726148822 tensor(0.0079)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.010538735426962376 0.985855013892397 0.9689672293942403 tensor(0.0122)\n",
      "total_card:  torch.Size([128])\n",
      "miss_card:  torch.Size([128])\n",
      "0.011180868372321129 0.988537880905787 0.9637503406922867 tensor(0.0323)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a20c1e944ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print (targets[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-dff79b58f867>\u001b[0m in \u001b[0;36mprint_loss\u001b[0;34m(estimates, targets, cards)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mfalse_positive\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mfalse_negative\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, thresholds, distances, targets, cards) in enumerate(test_loader):\n",
    "    x = Variable(features)\n",
    "    y = Variable(targets.unsqueeze(1))\n",
    "    z = Variable(thresholds)\n",
    "    dist = Variable(distances)\n",
    "    estimates = model(x, dist, z)\n",
    "    loss = loss_fn(estimates, targets, cards)\n",
    "    prec, rec, miss_rate = print_loss(estimates, targets, cards)\n",
    "    print (loss.item(), prec, rec, miss_rate)\n",
    "#     print (targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#     torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_dataset = np.array(f['train'])\n",
    "test_dataset = np.array(f['test'])\n",
    "train_lefts, train_rights, test_lefts, test_rights = prepare_dataset(train_dataset, test_dataset, train_num, test_num)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    (train_lefts, train_rights), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    (test_lefts, test_rights), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_distances, input_distances = test(model, device, train_loader)\n",
    "hash_distances, input_distances = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = torch.FloatTensor([f['train'][0] for x in range(999)])\n",
    "rights = torch.FloatTensor(f['train'][1:1000])\n",
    "inputdistance = angular_distance(lefts, rights).detach().numpy()\n",
    "hashdistance = l1_distance(model(lefts), model(rights)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xx in zip(inputdistance, hashdistance):\n",
    "#     print (xx[0], xx[1])\n",
    "index_1 = np.argsort(hashdistance, 0)\n",
    "index_2 = np.argsort(inputdistance, 0)\n",
    "# np.random.shuffle(index_2)\n",
    "\n",
    "input_index = {}\n",
    "for pos, idx in enumerate(index_2):\n",
    "    input_index[idx] = pos\n",
    "sum = 0.0\n",
    "for pos, idx in enumerate(index_1):\n",
    "    sum += np.abs(pos - input_index[idx])\n",
    "sum / len(index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.sort(inputdistance, 0)\n",
    "plt.plot(xxx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "distances = []\n",
    "for i in index_1:\n",
    "    distances.append(math.floor(inputdistance[i].item()* 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(torch.FloatTensor(f['train'][0]).unsqueeze(0), torch.FloatTensor(f['train'][6]).unsqueeze(0), dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(hash_distances[0][0:30], input_distances[0][0:30]):\n",
    "    print (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vector = model(torch.FloatTensor(f['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = model(torch.FloatTensor(f['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(vector):\n",
    "    query_codes = []\n",
    "    for v in vector:\n",
    "        binary_code = []\n",
    "        for e in v:\n",
    "            if e < 0.5:\n",
    "                binary_code.append(0)\n",
    "            else:\n",
    "                binary_code.append(1)\n",
    "        query_codes.append(binary_code)\n",
    "    return np.array(query_codes)\n",
    "dataset_binary = binarization(dataset_vector.detach().numpy())\n",
    "query_binary = binarization(query_vector.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "hash_table = {}\n",
    "for idx, point in enumerate(dataset_binary):\n",
    "    pos = 0\n",
    "    key = 0\n",
    "    for d in point:\n",
    "        key += d * math.pow(2, pos)\n",
    "        pos += 1\n",
    "    if key in hash_table:\n",
    "        hash_table[key].append(idx)\n",
    "    else:\n",
    "        hash_table[key] = [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['neighbors'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidate_distance(vector, hash_table, candidate_num):\n",
    "    candidate = []\n",
    "    for point in query_binary:\n",
    "        cand = []\n",
    "        dis = 0\n",
    "        while len(cand) < 100:\n",
    "            pos = 0\n",
    "            key = 0\n",
    "            for d in point:\n",
    "                key += d * math.pow(2, pos)\n",
    "                pos += 1\n",
    "            if key in hash_table:\n",
    "                candidate.append(hash_table[key])\n",
    "    return candidate\n",
    "find_candidate_0_distance(query_binary, hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(hash_code, data_index_set):\n",
    "        self.hash_code = hash_code\n",
    "        self.data_index_set = data_index_set\n",
    "        self.children = []\n",
    "        \n",
    "    def isLeaf():\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def train(dataset):\n",
    "        train_data = dataset[self.data_index_set]\n",
    "        self.model = train(dataset)\n",
    "        \n",
    "    def partition():\n",
    "        points = dataset[self.data_index_set]\n",
    "        hash_table = {}\n",
    "        codes = self.model(points)\n",
    "        for idx, code in enumerate(codes):\n",
    "            if code in hash_table:\n",
    "                hash_table[code].append(self.data_index_set[idx])\n",
    "            else:\n",
    "                hash_table[code] = [self.data_index_set[idx]]\n",
    "        for key,value in d.items():\n",
    "            self.children.append(Node(key, value))\n",
    "    \n",
    "    def search(query, dataset):\n",
    "        if self.isLeaf():\n",
    "            return validate(dataset[self.data_index_set])\n",
    "        else:\n",
    "            children_idxes = select_children(query)\n",
    "            result = []\n",
    "            for idx in children_idxes:\n",
    "                result += self.children[idx].search(query, dataset)\n",
    "            return result\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def index_construction(dataset):\n",
    "    model = train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "data = np.array(f['train'])\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
    "X_tsne = tsne.fit_transform(data[np.random.choice(data.shape[0], 100000, replace=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
